<html>
<head><meta charset="utf-8"><title>[isabelle] Isabelle2020-RC5 is now available --- Isabelle... · Archive Mirror: Isabelle Users Mailing List · Zulip Chat Archive</title></head>
<h2>Stream: <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/index.html">Archive Mirror: Isabelle Users Mailing List</a></h2>
<h3>Topic: <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/topic/.5Bisabelle.5D.20Isabelle2020-RC5.20is.20now.20available.20---.20Isabelle.2E.2E.2E.html">[isabelle] Isabelle2020-RC5 is now available --- Isabelle...</a></h3>

<hr>

<base href="https://isabelle.zulipchat.com/">

<head><link href="http://isabelle.systems/zulip-archive/style.css" rel="stylesheet"></head>

<a name="294826910"></a>
<h4><a href="https://isabelle.zulipchat.com/#narrow/stream/336180-Archive%20Mirror%3A%20Isabelle%20Users%20Mailing%20List/topic/%5Bisabelle%5D%20Isabelle2020-RC5%20is%20now%20available%20---%20Isabelle.../near/294826910" class="zl"><img src="http://isabelle.systems/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Email Gateway <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/topic/.5Bisabelle.5D.20Isabelle2020-RC5.20is.20now.20available.20---.20Isabelle.2E.2E.2E.html#294826910">(Aug 23 2022 at 08:54)</a>:</h4>
<p>From: Makarius &lt;<a href="mailto:makarius@sketis.net">makarius@sketis.net</a>&gt;<br>
On 12/04/2020 20:53, Peter Lammich wrote:</p>
<blockquote>
<p>I was interrupted by a "Prover process<br>
terminated with error" dialogue displaying the message below.</p>
<p>Welcome to Isabelle/HOL (Isabelle2020-RC5: April 2020)<br>
Interrupt<br>
Interrupt<br>
Interrupt<br>
standard_error terminated<br>
standard_output terminated<br>
process terminated<br>
Malformed message:<br>
bad chunk (unexpected EOF after 679 of 836 bytes)<br>
message_output terminated<br>
command_input terminated<br>
process_manager terminated<br>
Return code: 137</p>
</blockquote>
<p>The return code 137 = 128 + 9 means that the poly process was terminated by a<br>
hard kill, i.e. the runtime system did not react to anymore to SIGINT or SIGTERM.</p>
<p>Moreover, the Interrupt exceptions on the syslog look suspicious: it indicates<br>
resource problems in ML, e.g. severe shortage of heap or stack.</p>
<p>What is the output the following?</p>
<p>* isabelle getenv ML_PLATFORM ML_OPTIONS</p>
<p>* ulimit -a</p>
<blockquote>
<p>On the command line, I see</p>
<p>j65266pl@cspool450:~/devel/isabelle/RF2/llvm$ ~/opt/Isabelle2020-<br>
RC5/bin/isabelle jedit -d ~/devel/isabelle/afp-devel/thys/<br>
basic/LLVM_Basic_Main.thy</p>
</blockquote>
<p>Where can I get Isabelle/LLVM for Isabelle2020?</p>
<p>Makarius</p>



<a name="294826923"></a>
<h4><a href="https://isabelle.zulipchat.com/#narrow/stream/336180-Archive%20Mirror%3A%20Isabelle%20Users%20Mailing%20List/topic/%5Bisabelle%5D%20Isabelle2020-RC5%20is%20now%20available%20---%20Isabelle.../near/294826923" class="zl"><img src="http://isabelle.systems/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Email Gateway <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/topic/.5Bisabelle.5D.20Isabelle2020-RC5.20is.20now.20available.20---.20Isabelle.2E.2E.2E.html#294826923">(Aug 23 2022 at 08:54)</a>:</h4>
<p>From: Peter Lammich &lt;<a href="mailto:lammich@in.tum.de">lammich@in.tum.de</a>&gt;</p>
<blockquote>
<p>What is the output the following?</p>
<p>* isabelle getenv ML_PLATFORM ML_OPTIONS</p>
<p>* ulimit -a</p>
</blockquote>
<p>j65266pl@cspool450:~$ ~/opt/Isabelle2020-RC5/bin/isabelle getenv<br>
ML_PLATFORM ML_OPTIONS<br>
ML_PLATFORM=x86_64_32-linux<br>
ML_OPTIONS=--minheap 500<br>
j65266pl@cspool450:~$ ulimit -a<br>
core file size          (blocks, -c) 0<br>
data seg size           (kbytes, -d) unlimited<br>
scheduling priority             (-e) 0<br>
file size               (blocks, -f) unlimited<br>
pending signals                 (-i) 127434<br>
max locked memory       (kbytes, -l) 16384<br>
max memory size         (kbytes, -m) unlimited<br>
open files                      (-n) 1024<br>
pipe size            (512 bytes, -p) 8<br>
POSIX message queues     (bytes, -q) 819200<br>
real-time priority              (-r) 0<br>
stack size              (kbytes, -s) 8192<br>
cpu time               (seconds, -t) unlimited<br>
max user processes              (-u) 127434<br>
virtual memory          (kbytes, -v) unlimited<br>
file locks                      (-x) unlimited</p>
<blockquote>
<p>Where can I get Isabelle/LLVM for Isabelle2020?</p>
</blockquote>
<p><a href="https://github.com/lammich/isabelle_llvm">https://github.com/lammich/isabelle_llvm</a></p>
<p>However, the session that failed used a private copy with some<br>
experimental changes. Anyway, the error seems not reproducible.</p>



<a name="294826948"></a>
<h4><a href="https://isabelle.zulipchat.com/#narrow/stream/336180-Archive%20Mirror%3A%20Isabelle%20Users%20Mailing%20List/topic/%5Bisabelle%5D%20Isabelle2020-RC5%20is%20now%20available%20---%20Isabelle.../near/294826948" class="zl"><img src="http://isabelle.systems/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Email Gateway <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/topic/.5Bisabelle.5D.20Isabelle2020-RC5.20is.20now.20available.20---.20Isabelle.2E.2E.2E.html#294826948">(Aug 23 2022 at 08:54)</a>:</h4>
<p>From: Makarius &lt;<a href="mailto:makarius@sketis.net">makarius@sketis.net</a>&gt;<br>
On 13/04/2020 17:44, Peter Lammich wrote:</p>
<blockquote>
<blockquote>
<p>What is the output the following?</p>
<p>* isabelle getenv ML_PLATFORM ML_OPTIONS</p>
<p>* ulimit -a</p>
</blockquote>
<p>j65266pl@cspool450:~$ ~/opt/Isabelle2020-RC5/bin/isabelle getenv<br>
ML_PLATFORM ML_OPTIONS<br>
ML_PLATFORM=x86_64_32-linux<br>
ML_OPTIONS=--minheap 500<br>
j65266pl@cspool450:~$ ulimit -a<br>
core file size          (blocks, -c) 0<br>
data seg size           (kbytes, -d) unlimited<br>
scheduling priority             (-e) 0<br>
file size               (blocks, -f) unlimited<br>
pending signals                 (-i) 127434<br>
max locked memory       (kbytes, -l) 16384<br>
max memory size         (kbytes, -m) unlimited<br>
open files                      (-n) 1024<br>
pipe size            (512 bytes, -p) 8<br>
POSIX message queues     (bytes, -q) 819200<br>
real-time priority              (-r) 0<br>
stack size              (kbytes, -s) 8192<br>
cpu time               (seconds, -t) unlimited<br>
max user processes              (-u) 127434<br>
virtual memory          (kbytes, -v) unlimited<br>
file locks                      (-x) unlimited</p>
</blockquote>
<p>The parameters are fine -- the usual defaults on Linux.</p>
<blockquote>
<blockquote>
<p>Where can I get Isabelle/LLVM for Isabelle2020?</p>
</blockquote>
<p><a href="https://github.com/lammich/isabelle_llvm">https://github.com/lammich/isabelle_llvm</a></p>
<p>However, the session that failed used a private copy with some<br>
experimental changes. Anyway, the error seems not reproducible.</p>
</blockquote>
<p>The point is the get an impression about overall resource usage. After playing<br>
with it for 30min, it looks all fine and smooth to me.</p>
<p>There is significant usage of ML heap, though. On a mobile machine with 16GB<br>
total, the heap converges towards 10-11GB.</p>
<p>I have occasionally seen instabilities of the Poly/ML runtime system in batch<br>
builds in such rather full situations.</p>
<p>For now, I would say we keep watching it carefully. David Matthews has done a<br>
lot of fine-tuning of heap management recently, and a bit more is to be<br>
expected soon (after the Isabelle2020 release).</p>
<p>Makarius</p>



<a name="294826959"></a>
<h4><a href="https://isabelle.zulipchat.com/#narrow/stream/336180-Archive%20Mirror%3A%20Isabelle%20Users%20Mailing%20List/topic/%5Bisabelle%5D%20Isabelle2020-RC5%20is%20now%20available%20---%20Isabelle.../near/294826959" class="zl"><img src="http://isabelle.systems/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Email Gateway <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/topic/.5Bisabelle.5D.20Isabelle2020-RC5.20is.20now.20available.20---.20Isabelle.2E.2E.2E.html#294826959">(Aug 23 2022 at 08:55)</a>:</h4>
<p>From: Peter Lammich &lt;<a href="mailto:lammich@in.tum.de">lammich@in.tum.de</a>&gt;</p>
<blockquote>
<p>There is significant usage of ML heap, though. On a mobile machine<br>
with 16GB<br>
total, the heap converges towards 10-11GB.</p>
<p>I have occasionally seen instabilities of the Poly/ML runtime system<br>
in batch<br>
builds in such rather full situations.</p>
</blockquote>
<p>I have 32GB of memory.</p>
<blockquote>
<p>For now, I would say we keep watching it carefully. David Matthews<br>
has done a<br>
lot of fine-tuning of heap management recently, and a bit more is to<br>
be<br>
expected soon (after the Isabelle2020 release).</p>
</blockquote>
<p>Ok, I'll watch it. Right now, it looks that it was a one-in-a-million<br>
thing, as it did not (yet) occur again.</p>



<a name="294826968"></a>
<h4><a href="https://isabelle.zulipchat.com/#narrow/stream/336180-Archive%20Mirror%3A%20Isabelle%20Users%20Mailing%20List/topic/%5Bisabelle%5D%20Isabelle2020-RC5%20is%20now%20available%20---%20Isabelle.../near/294826968" class="zl"><img src="http://isabelle.systems/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Email Gateway <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/topic/.5Bisabelle.5D.20Isabelle2020-RC5.20is.20now.20available.20---.20Isabelle.2E.2E.2E.html#294826968">(Aug 23 2022 at 08:55)</a>:</h4>
<p>From: Makarius &lt;<a href="mailto:makarius@sketis.net">makarius@sketis.net</a>&gt;<br>
OK, I also have 32GB on my main machine and will continue testing with that.</p>
<p>Note that Poly/ML in x86_64_32 mode (default) is limited to approx. 16GB, and<br>
these 10-11GB user heap are rather typical in practice. David Matthews has<br>
some ideas in the pipeline to double that space --- it will be flushed the<br>
next time there is sufficient funding for such a project.</p>
<p>Which base logic image are you typically using for editing Isabelle/LLVM?</p>
<p>Makarius</p>



<a name="294826982"></a>
<h4><a href="https://isabelle.zulipchat.com/#narrow/stream/336180-Archive%20Mirror%3A%20Isabelle%20Users%20Mailing%20List/topic/%5Bisabelle%5D%20Isabelle2020-RC5%20is%20now%20available%20---%20Isabelle.../near/294826982" class="zl"><img src="http://isabelle.systems/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Email Gateway <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/topic/.5Bisabelle.5D.20Isabelle2020-RC5.20is.20now.20available.20---.20Isabelle.2E.2E.2E.html#294826982">(Aug 23 2022 at 08:55)</a>:</h4>
<p>From: Peter Lammich &lt;<a href="mailto:lammich@in.tum.de">lammich@in.tum.de</a>&gt;</p>
<blockquote>
<p>Which base logic image are you typically using for editing<br>
Isabelle/LLVM?</p>
</blockquote>
<p>Typically, afp/Refine_Monadic.</p>
<p>When the error happened, I had only loaded vcg/LLVM_VCG_Main and <br>
a few other theories I'm currently working on, from the default HOL<br>
image.</p>
<p>Peter</p>
<blockquote>
<p>Makarius<br>
</p>
</blockquote>



<a name="294827224"></a>
<h4><a href="https://isabelle.zulipchat.com/#narrow/stream/336180-Archive%20Mirror%3A%20Isabelle%20Users%20Mailing%20List/topic/%5Bisabelle%5D%20Isabelle2020-RC5%20is%20now%20available%20---%20Isabelle.../near/294827224" class="zl"><img src="http://isabelle.systems/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Email Gateway <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/topic/.5Bisabelle.5D.20Isabelle2020-RC5.20is.20now.20available.20---.20Isabelle.2E.2E.2E.html#294827224">(Aug 23 2022 at 08:57)</a>:</h4>
<p>From: Peter Lammich &lt;<a href="mailto:lammich@in.tum.de">lammich@in.tum.de</a>&gt;<br>
Hi, <br>
I got the error again, and this time I investigated it a bit. Linux<br>
invoked OOM-killer on the poly-process! I attached the log, the last 3<br>
messages show which process was selected for killing.There were two<br>
Isabelle's running, and OOM-killer picked one.<br>
Can I somehow tell poly to do more aggressive garbage collection,<br>
BEFORE it gets OOM killed?<br>
What changed in the poly configuration? I never had those problems with<br>
Isabelle-2019, and sometimes had up to 3 instances running in parallel.<br>
How can I find out if I changed something? A diff between the<br>
.isabelle-folders reveals nothing suspicious to me?</p>
<p>Best,  Peter</p>
<p>------------------Apr 16 23:58:12 cspool450<br>
org.gnome.Shell.desktop[3621]: Memory pressure relief: Total: res =<br>
34840576/34803712/-36864, res+swap = 28217344/28217344/0Apr 16 23:58:17<br>
cspool450 kernel: PacerThread invoked oom-killer:<br>
gfp_mask=0x100cca(GFP_HIGHUSER_MOVABLE), order=0, oom_score_adj=300Apr<br>
16 23:58:17 cspool450 kernel: CPU: 6 PID: 28464 Comm: PacerThread Not<br>
tainted 5.3.0-46-generic #38~18.04.1-UbuntuApr 16 23:58:17 cspool450<br>
kernel: Hardware name: Dell Inc. Precision 3540/0M14W7, BIOS 1.6.5<br>
12/26/2019Apr 16 23:58:17 cspool450 kernel: Call Trace:Apr 16 23:58:17<br>
cspool450 kernel:  dump_stack+0x6d/0x95Apr 16 23:58:17 cspool450<br>
kernel:  dump_header+0x4f/0x200Apr 16 23:58:17 cspool450<br>
kernel:  oom_kill_process+0xe6/0x120Apr 16 23:58:17 cspool450<br>
kernel:  out_of_memory+0x109/0x510Apr 16 23:58:17 cspool450<br>
kernel:  __alloc_pages_slowpath+0xad1/0xe10Apr 16 23:58:17 cspool450<br>
kernel:  ? __switch_to_asm+0x34/0x70Apr 16 23:58:17 cspool450<br>
kernel:  __alloc_pages_nodemask+0x2cd/0x320Apr 16 23:58:17 cspool450<br>
kernel:  alloc_pages_current+0x6a/0xe0Apr 16 23:58:17 cspool450<br>
kernel:  __page_cache_alloc+0x6a/0xa0Apr 16 23:58:17 cspool450<br>
kernel:  pagecache_get_page+0x9c/0x2b0Apr 16 23:58:17 cspool450<br>
kernel:  filemap_fault+0x3a2/0xb00Apr 16 23:58:17 cspool450 kernel:  ?<br>
unlock_page_memcg+0x12/0x20Apr 16 23:58:17 cspool450 kernel:  ?<br>
page_add_file_rmap+0x5e/0x150Apr 16 23:58:17 cspool450 kernel:  ?<br>
alloc_set_pte+0x52e/0x5f0Apr 16 23:58:17 cspool450 kernel:  ?<br>
filemap_map_pages+0x18f/0x380Apr 16 23:58:17 cspool450<br>
kernel:  ext4_filemap_fault+0x31/0x44Apr 16 23:58:17 cspool450<br>
kernel:  __do_fault+0x57/0x117Apr 16 23:58:17 cspool450<br>
kernel:  __handle_mm_fault+0xda1/0x1230Apr 16 23:58:17 cspool450<br>
kernel:  handle_mm_fault+0xcb/0x210Apr 16 23:58:17 cspool450<br>
kernel:  __do_page_fault+0x2a1/0x4d0Apr 16 23:58:17 cspool450<br>
kernel:  do_page_fault+0x2c/0xe0Apr 16 23:58:17 cspool450<br>
kernel:  page_fault+0x34/0x40Apr 16 23:58:17 cspool450 kernel: RIP:<br>
0033:0x5566f01d66a2Apr 16 23:58:17 cspool450 kernel: Code: Bad RIP<br>
value.Apr 16 23:58:17 cspool450 kernel: RSP: 002b:00007fb61fad6790<br>
EFLAGS: 00010206Apr 16 23:58:17 cspool450 kernel: RAX: 00005566eba0ba18<br>
RBX: 7fffffffffffffff RCX: 00005566eba0ba18Apr 16 23:58:17 cspool450<br>
kernel: RDX: 0000132de75dc948 RSI: 000000000001d4ae RDI:<br>
0000132de75dc8f0Apr 16 23:58:17 cspool450 kernel: RBP: 00007fb61fad67b0<br>
R08: 0000000000022580 R09: 00007fb61fad6630Apr 16 23:58:17 cspool450<br>
kernel: R10: 00007fb61fad65f0 R11: 0000000000000206 R12:<br>
0000132de75dc8f0Apr 16 23:58:17 cspool450 kernel: R13: 00000000000495d4<br>
R14: 0000132de75dc8f0 R15: 00000020c0b47fedApr 16 23:58:17 cspool450<br>
kernel: Mem-Info:Apr 16 23:58:17 cspool450 kernel: active_anon:7390182<br>
inactive_anon:571000<br>
isolated_anon:0                                   active_file:276<br>
inactive_file:1<br>
isolated_file:0                                   unevictable:25173<br>
dirty:1 writeback:0<br>
unstable:0                                   slab_reclaimable:25025<br>
slab_unreclaimable:37942                                   mapped:23813<br>
7 shmem:516419 pagetables:38113<br>
bounce:0                                   free:50911 free_pcp:2186<br>
free_cma:0Apr 16 23:58:17 cspool450 kernel: Node 0<br>
active_anon:29560728kB inactive_anon:2284000kB active_file:1104kB<br>
inactive_file:4kB unevictable:100692kB isolated(anon):0kB<br>
isolated(file):0kB mapped:952548kB dirty:4kB writeback:0kB<br>
shmem:2065676kB shmem_thp: 0kB shmem_pmdmapped: 0kB anon_thp: 0kB<br>
writeback_tmp:0kB unstable:0kB all_unreclaimable? noApr 16 23:58:17<br>
cspool450 kernel: Node 0 DMA free:15900kB min:32kB low:44kB high:56kB<br>
active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB<br>
unevictable:0kB writepending:0kB present:15992kB managed:15900kB<br>
mlocked:0kB kernel_stack:0kB pagetables:0kB bounce:0kB free_pcp:0kB<br>
local_pcp:0kB free_cma:0kBApr 16 23:58:17 cspool450 kernel:<br>
lowmem_reserve[]: 0 1773 31843 31843 31843Apr 16 23:58:17 cspool450<br>
kernel: Node 0 DMA32 free:124016kB min:3760kB low:5576kB high:7392kB<br>
active_anon:1392156kB inactive_anon:355744kB active_file:76kB<br>
inactive_file:0kB unevictable:3132kB writepending:0kB present:1948512kB<br>
managed:1882692kB mlocked:0kB kernel_stack:0kB pagetables:316kB<br>
bounce:0kB free_pcp:112kB local_pcp:4kB free_cma:0kBApr 16 23:58:17<br>
cspool450 kernel: lowmem_reserve[]: 0 0 30069 30069 30069Apr 16<br>
23:58:17 cspool450 kernel: Node 0 Normal free:63728kB min:63788kB<br>
low:94576kB high:125364kB active_anon:28168572kB<br>
inactive_anon:1928256kB active_file:484kB inactive_file:612kB<br>
unevictable:97560kB writepending:4kB present:31399936kB<br>
managed:30799340kB mlocked:264kB kernel_stack:24384kB<br>
pagetables:152136kB bounce:0kB free_pcp:8632kB local_pcp:1032kB<br>
free_cma:0kBApr 16 23:58:17 cspool450 kernel: lowmem_reserve[]: 0 0 0 0<br>
0Apr 16 23:58:17 cspool450 kernel: Node 0 DMA: 1<em>4kB (U) 1</em>8kB (U)<br>
1<em>16kB (U) 2</em>32kB (U) 1<em>64kB (U) 1</em>128kB (U) 1<em>256kB (U) 0</em>512kB<br>
1<em>1024kB (U) 1</em>2048kB (M) 3*4096kB (M) = 15900kBApr 16 23:58:17<br>
cspool450 kernel: Node 0 DMA32: 348<em>4kB (UME) 313</em>8kB (UME) 265*16kB<br>
(UME) 324<em>32kB (UME) 334</em>64kB (UME) 288<em>128kB (UME) 89</em>256kB (UME)<br>
45<em>512kB (UME) 2</em>1024kB (UM) 0<em>2048kB 0</em>4096kB = 124616kBApr 16<br>
23:58:17 cspool450 kernel: Node 0 Normal: 5111<em>4kB (UM) 1671</em>8kB (UME)<br>
490<em>16kB (UME) 298</em>32kB (UME) 219<em>64kB (UME) 0</em>128kB 0<em>256kB 0</em>512kB<br>
0<em>1024kB 0</em>2048kB 0*4096kB = 65204kBApr 16 23:58:17 cspool450 kernel:<br>
Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0<br>
hugepages_size=1048576kBApr 16 23:58:17 cspool450 kernel: Node 0<br>
hugepages_total=0 hugepages_free=0 hugepages_surp=0<br>
hugepages_size=2048kBApr 16 23:58:17 cspool450 kernel: 672338 total<br>
pagecache pagesApr 16 23:58:17 cspool450 kernel: 155339 pages in swap<br>
cacheApr 16 23:58:17 cspool450 kernel: Swap cache stats: add 1433360,<br>
delete 1278008, find 462505/467558Apr 16 23:58:17 cspool450 kernel:<br>
Free swap  = 0kBApr 16 23:58:17 cspool450 kernel: Total swap =<br>
999420kBApr 16 23:58:17 cspool450 kernel: 8341110 pages RAMApr 16<br>
23:58:17 cspool450 kernel: 0 pages HighMem/MovableOnlyApr 16 23:58:17<br>
cspool450 kernel: 166627 pages reservedApr 16 23:58:17 cspool450<br>
kernel: 0 pages cma reservedApr 16 23:58:17 cspool450 kernel: 0 pages<br>
hwpoisonedApr 16 23:58:17 cspool450 kernel: Tasks state (memory values<br>
in pages):Apr 16 23:58:17 cspool450 kernel: [  pid  ]   uid  tgid<br>
total_vm      rss pgtables_bytes swapents oom_score_adj nameApr 16<br>
23:58:17 cspool450 kernel:<br>
[    564]     0   564    45374      349   380928        0             0<br>
systemd-journalApr 16 23:58:17 cspool450 kernel:<br>
[    574]     0   574    26475       56    94208        0             0<br>
lvmetadApr 16 23:58:17 cspool450 kernel:<br>
[    582]     0   582    11752      513   122880        0         -1000 <br>
systemd-udevdApr 16 23:58:17 cspool450 kernel:<br>
[    889]   101   889    17827      318   172032        0             0<br>
systemd-resolveApr 16 23:58:17 cspool450 kernel: [    891]<br>
62583   891    36528      141   192512        0             0 systemd-<br>
timesynApr 16 23:58:17 cspool450 kernel:<br>
[    988]     0   988    17708      232   180224        0             0<br>
systemd-logindApr 16 23:58:17 cspool450 kernel:<br>
[    990]     0   990    74256      194   208896        0             0<br>
boltdApr 16 23:58:17 cspool450 kernel:<br>
[    997]     0   997   108599      396   356352        0             0<br>
ModemManagerApr 16 23:58:17 cspool450 kernel:<br>
[    998]     0   998    44408     1990   241664        0             0<br>
networkd-dispatApr 16 23:58:17 cspool450 kernel:<br>
[    999]     0   999    27637       95   114688        0             0<br>
irqbalanceApr 16 23:58:17 cspool450 kernel:<br>
[   1001]     0  1001    46749      209   196608        0             0<br>
thermaldApr 16 23:58:17 cspool450 kernel:<br>
[   1006]     0  1006   125919      656   348160        0             0<br>
udisksdApr 16 23:58:17 cspool450 kernel:<br>
[   1017]   116  1017    11815       86   126976        0             0<br>
avahi-daemonApr 16 23:58:17 cspool450 kernel:<br>
[   1022]   102  1022    65758      325   167936        0             0<br>
rsyslogdApr 16 23:58:17 cspool450 kernel:<br>
[   1025]     0  1025     9123      102   114688        0             0<br>
bluetoothdApr 16 23:58:17 cspool450 kernel:<br>
[   1031]     0  1031     9606       74   118784        0             0<br>
cronApr 16 23:58:17 cspool450 kernel:<br>
[   1035]   103  1035    13205      685   143360        0          -900 <br>
dbus-daemonApr 16 23:58:17 cspool450 kernel:<br>
[   1047]   116  1047    11768       86   122880        0             0<br>
avahi-daemonApr 16 23:58:17 cspool450 kernel:<br>
[   1096]     0  1096    11431      280   126976        0             0<br>
wpa_supplicantApr 16 23:58:17 cspool450 kernel:<br>
[   1099]     0  1099    73693      262   208896        0             0<br>
accounts-daemonApr 16 23:58:17 cspool450 kernel:<br>
[   1102]     0  1102     1137       16    53248        0             0<br>
acpidApr 16 23:58:17 cspool450 kernel:<br>
[   1105]     0  1105   160255     1235   466944        0             0<br>
NetworkManagerApr 16 23:58:17 cspool450 kernel:<br>
[   1111]     0  1111   548623     2902   425984        0          -900 <br>
snapdApr 16 23:58:17 cspool450 kernel:<br>
[   1390]     0  1390    75962      891   237568        0             0<br>
polkitdApr 16 23:58:17 cspool450 kernel:<br>
[   1643]   117  1643    81510     1538   266240        0             0<br>
colordApr 16 23:58:17 cspool450 kernel:<br>
[   1690]     0  1690    48585     1972   26624<br>
[message truncated]</p>



<a name="294827270"></a>
<h4><a href="https://isabelle.zulipchat.com/#narrow/stream/336180-Archive%20Mirror%3A%20Isabelle%20Users%20Mailing%20List/topic/%5Bisabelle%5D%20Isabelle2020-RC5%20is%20now%20available%20---%20Isabelle.../near/294827270" class="zl"><img src="http://isabelle.systems/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Email Gateway <a href="http://isabelle.systems/zulip-archive/stream/336180-Archive-Mirror.3A-Isabelle-Users-Mailing-List/topic/.5Bisabelle.5D.20Isabelle2020-RC5.20is.20now.20available.20---.20Isabelle.2E.2E.2E.html#294827270">(Aug 23 2022 at 08:57)</a>:</h4>
<p>From: Makarius &lt;<a href="mailto:makarius@sketis.net">makarius@sketis.net</a>&gt;<br>
On 17/04/2020 01:12, Peter Lammich wrote:</p>
<blockquote>
<p>I got the error again, and this time I investigated it a bit. Linux invoked<br>
OOM-killer on the poly-process!<br>
I attached the log, the last 3 messages show which process was selected for<br>
killing.<br>
There were two Isabelle's running, and OOM-killer picked one.</p>
</blockquote>
<p>I did not know anything about the OOM killer yet, but it seems to cause<br>
problems occasionally. E.g. see<br>
<a href="https://serverfault.com/questions/101916/turn-off-the-linux-oom-killer-by-default?rq=1">https://serverfault.com/questions/101916/turn-off-the-linux-oom-killer-by-default?rq=1</a></p>
<blockquote>
<p>Can I somehow tell poly to do more aggressive garbage collection, BEFORE it<br>
gets OOM killed?</p>
</blockquote>
<p>You need to ask that David Matthews.</p>
<p>Maybe you just need more swap space.</p>
<p>Or you should limit the poly process explicitly, e.g. in<br>
$ISABELLE_HOME_USER/etc/settings:</p>
<p>ML_OPTIONS="--minheap 1500 --maxheap 8g"</p>
<p>Makarius</p>



<hr><p>Last updated: Nov 01 2025 at 16:22 UTC</p>
</html>