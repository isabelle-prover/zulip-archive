[
    {
        "content": "<p>From: <a href=\"mailto:hannobecker@posteo.de\">hannobecker@posteo.de</a><br>\nHi,</p>\n<p>I am working on a large development for which continuous proof checking <br>\nin Isabelle/jEdit gets increasingly slow on my local machine.</p>\n<p>(How) Can one offload continuous proof checking to a remote server, <br>\nwhile continuing to run jEdit locally? (Switching to VSCode is also an <br>\noption if things are more flexible there).</p>\n<p>Thanks,<br>\nHanno</p>",
        "id": 381361328,
        "sender_full_name": "Email Gateway",
        "timestamp": 1691041856
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nNote that Isabelle/VSCode is just an experimental PIDE front-end, and still <br>\ntwo orders of magnitude behind Isabelle/jEdit in sophistication.</p>\n<p>Concerning remote checking: that has been vital many decades ago, and today it <br>\nre-occurs in rare situations. Can you be more specific, what your application <br>\nis like? How many CPU cores and how many GB of RAM does it usually require?</p>\n<p>And how many CPU cores and GB RAM does your local machine have? 4-8 cores and <br>\n16-32 GB RAM should be considered normal for regular Isabelle applications, <br>\nand no reason for the extra complexity of remote computation.</p>\n<p>In contrast, my impression is that the primary importance of remote checking <br>\nis to build huge library sessions in the background (e.g. portions of AFP), <br>\nwhile only a relatively small application is edited in the foreground (e.g. <br>\nthe increment wrt. the background library).</p>\n<p>Makarius</p>",
        "id": 383000559,
        "sender_full_name": "Email Gateway",
        "timestamp": 1691509048
    },
    {
        "content": "<p>From: Dominic Mulligan &lt;<a href=\"mailto:cl-isabelle-users@lists.cam.ac.uk\">cl-isabelle-users@lists.cam.ac.uk</a>&gt;<br>\nWe largely develop locally on Apple M1 MacBook Pros with 32GB RAM.<br>\nProof-checking time is now approaching an hour on these machines<br>\ndespite significant effort being expended in trying to optimize our<br>\nautomation setup to keep builds as efficient as possible.  However, we also<br>\nhave access to machines with 192 cores and terabytes of RAM—and every other<br>\nmachine configuration imaginable—hence the motivation for Hanno's question.</p>",
        "id": 383614684,
        "sender_full_name": "Email Gateway",
        "timestamp": 1691672127
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nLarge numbers of cores and memory may sound important, but scaling is never <br>\nfor free, and certainly not linear.</p>\n<p>Such server class hardware is usually targeted at \"cloud\" services, with many <br>\nindependent workloads and relative modest (virtual) hardware parameters.</p>\n<p>It depends on CPU speed, caches, and NUMA memory architecture, how much of <br>\nperformance you can get for Isabelle. For a single ML process, it is very hard <br>\nto go beyond 16 cores and 64 GB RAM: it will become slow and unresponsive (due <br>\nto excessive GC).</p>\n<p>High-end consumer machines often work better. like the current Macbook Pro M2 <br>\nor M2 max.</p>\n<p>With the updated version of Poly/ML in Isabelle2023-RC3, ARM64 should work <br>\nfine: please test it now.</p>\n<p>Makarius</p>",
        "id": 383765871,
        "sender_full_name": "Email Gateway",
        "timestamp": 1691704290
    },
    {
        "content": "<p>From: Dominic Mulligan &lt;<a href=\"mailto:cl-isabelle-users@lists.cam.ac.uk\">cl-isabelle-users@lists.cam.ac.uk</a>&gt;<br>\nHi,</p>",
        "id": 383776937,
        "sender_full_name": "Email Gateway",
        "timestamp": 1691708690
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOn 11/08/2023 01:04, Dominic Mulligan (via cl-isabelle-users Mailing List) wrote:</p>\n<blockquote>\n<p>As mentioned, we have access to any sort of machine configuration one could <br>\nreasonably imagine and resource constraints imposed by co-tenancy and <br>\nvirtualisation are not a concern in this context. Moreover, we can freely spin <br>\nup machines to test which ones work well and which ones do not, if needed, <br>\nempirically.</p>\n</blockquote>\n<p>Maybe you can run some concrete test with \"isabelle build\" to get some <br>\nmeasurements, and compare the results. It is rare that some \"cloud\" <br>\ninfrastructure is much faster than a high-end local machine.</p>\n<blockquote>\n<p>With the updated version of Poly/ML in Isabelle2023-RC3, ARM64 should work<br>\n    fine: please test it now.</p>\n<p>We tested RC2 and observed no performance boost, but rather a minor <br>\nperformance decrease in batch build benchmarking compared to Isabelle2022 on <br>\nour laptops. Is there reason to expect RC3 will be substantially different?</p>\n</blockquote>\n<p>Isabelle2023-RC3 is substantially different on ARM64: it used to crash <br>\noccasionally and people had to switch back to Intel x86_64 (which is a bit <br>\nslower).</p>\n<p>Now is the time to test and confirm that ARM64 works without side-conditions.</p>\n<blockquote>\n<p>(Note we will probably still update despite this, once the final release is <br>\nout, to take advantage of the maturing PolyML runtime for AArch64—as <br>\npreviously discussed on this list we have been observing long GC pauses and <br>\ndeadlocks in interactive editing with Isabelle2022 on AArch64.</p>\n</blockquote>\n<p>Time for testing is now. When the final release is out, there will be no <br>\nopportunity to change things until the next release.</p>\n<blockquote>\n<p>From your first reply to Hanno it seems that this distributed client/remote <br>\nserver mode of use of Isabelle is indeed possible and is also maybe currently <br>\nin use for large developments. Is there a handy example you could point to?</p>\n</blockquote>\n<p>I don't know of any concrete setup.</p>\n<p>Presently, I am still busy with remote batch-builds, to run \"isabelle build\" <br>\non many hosts in parallel. That introduces many technical side-conditions and <br>\npossibilities for failure, and will take some time to finish.</p>\n<p>Proper remote execution is not for free, but requires explicit efforts to make <br>\nit robust.</p>\n<p>Makarius</p>",
        "id": 383875641,
        "sender_full_name": "Email Gateway",
        "timestamp": 1691744155
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nFor Isabelle2023 there have been many changes in internal data structures for <br>\ncore concepts, e.g. name spaces. This usually saves ML heap space, and thus <br>\nindirectly GC time. It can also cost extra runtime elsewhere, but should <br>\nnormally be not that relevant.</p>\n<p>If you can point out particular problems, in the next 2 weeks before the <br>\nrelease becomes final, I might still be able to address them --- like I did in <br>\n<a href=\"https://isabelle.sketis.net/repos/isabelle-release/rev/3fdf3c5cfa9d\">https://isabelle.sketis.net/repos/isabelle-release/rev/3fdf3c5cfa9d</a></p>\n<p>Before the release things can be changed, after the release they can't.</p>\n<p>Makarius</p>",
        "id": 383878320,
        "sender_full_name": "Email Gateway",
        "timestamp": 1691744735
    },
    {
        "content": "<p>From: Dominic Mulligan &lt;<a href=\"mailto:cl-isabelle-users@lists.cam.ac.uk\">cl-isabelle-users@lists.cam.ac.uk</a>&gt;<br>\nHi,</p>\n<p>Isabelle2023-RC3 is substantially different on ARM64: it used to crash<br>\nOk, thanks for clarifying the status of RC2/RC3 for Arm.  We will give it a<br>\ngo.</p>\n<p>Thanks,<br>\nDominic</p>",
        "id": 383891437,
        "sender_full_name": "Email Gateway",
        "timestamp": 1691747449
    }
]