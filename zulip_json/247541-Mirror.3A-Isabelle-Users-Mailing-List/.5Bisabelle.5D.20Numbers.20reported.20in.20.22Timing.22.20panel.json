[
    {
        "content": "<p>From: \"Eugene W. Stark\" &lt;<a href=\"mailto:isabelle-users@starkeffect.com\">isabelle-users@starkeffect.com</a>&gt;<br>\nThere is something that I have noticed that makes optimizing the processing time of Isabelle theories more difficult:<br>\nThe timings reported in the \"Timings\" panel are highly variable, especially under near-thrashing conditions.<br>\nAn inference that can be performed in a few tens of ms can, under conditions of severe memory stress, be reported as<br>\ntaking many minutes.  This makes optimization difficult, as the timing results are not repeatable.</p>\n<p>I would like the timing numbers to only reflect actual computation time expended, not incidental delays that are<br>\ndependent on the memory conditions in which the particular inference is run.  It does not match my current goals to<br>\nspend time trying to read the source code to try to figure out why this occurs, but I am hoping that someone who already<br>\nunderstands the timing instrumentation code would be able to make simple modifications to make the results repeatable<br>\nand ultimately more useful.</p>\n<p>Thanks.</p>",
        "id": 207044524,
        "sender_full_name": "Email Gateway",
        "timestamp": 1597533625
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nI read from this observation that big Isabelle applications become more and<br>\nmore memory-bound: having too little heap space requires rather expensive<br>\noperations for garbage-collection and sharing of live data.</p>\n<p>David Matthews, the grand master behind Poly/ML, has recently provided more<br>\nfacilities for runtime statistics of memory management. I have already<br>\nintegrated some of this into the Isabelle/jEdit monitoring facilities (for the<br>\nnext release, presumably in Feb-2021). This confirms the impression that<br>\noverall heap usage is now more relevant than individual timing.</p>\n<p>How much memory do you actually have on your machine?</p>\n<p>For non-trivial applications 16 GB are already standard, and 32-64 GB are not<br>\nuncommon for really big things.</p>\n<p>Makarius</p>",
        "id": 208550221,
        "sender_full_name": "Email Gateway",
        "timestamp": 1598877026
    },
    {
        "content": "<p>From: \"Eugene W. Stark\" &lt;<a href=\"mailto:isabelle-users@starkeffect.com\">isabelle-users@starkeffect.com</a>&gt;<br>\nI have 24GB.</p>\n<p>I understand what you are saying, but I am asking for the timing numbers not to be confounded by non-CPU overhead.<br>\nThat way, even if I am running with a few extra browser tabs open I will see the same timing results and will still<br>\nknow whether what I am doing is optimizing something or making pointless changes.</p>\n<p>Thank you.</p>",
        "id": 208566951,
        "sender_full_name": "Email Gateway",
        "timestamp": 1598885343
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOn 31/08/2020 16:48, Eugene W. Stark wrote:</p>\n<blockquote>\n<p>I understand what you are saying, but I am asking for the timing numbers not to be confounded by non-CPU overhead.<br>\nThat way, even if I am running with a few extra browser tabs open I will see the same timing results and will still<br>\nknow whether what I am doing is optimizing something or making pointless changes.</p>\n</blockquote>\n<p>Isabelle as an application of symbolic logic mainly allocates tree data<br>\nstructures in memory, while gargabe collection deallocates them later on on.<br>\nThus it is inherently hard to isolate the \"non-CPU overhead\": it is the main<br>\npart of the program.</p>\n<blockquote>\n<p>I have 24GB.</p>\n</blockquote>\n<p>That it not much for the size of your typical applications on AFP.</p>\n<p>Do you see a chance to double the memory on that hardware?</p>\n<p>I have 32GB on my development machine, and don't do really big Isabelle<br>\napplications.</p>\n<p>Makarius</p>",
        "id": 208569407,
        "sender_full_name": "Email Gateway",
        "timestamp": 1598886261
    },
    {
        "content": "<p>From: \"Eugene W. Stark\" &lt;<a href=\"mailto:isabelle-users@starkeffect.com\">isabelle-users@starkeffect.com</a>&gt;<br>\nI could increase to 32GB on this hardware if I throw away some of the existing memory.<br>\nBut that is certainly not my point.</p>\n<p>An operating system (such as Linux, which is what I am using) provides system calls (cf getrusage(2),<br>\nclock_gettime(2)) that an application can use to measure resource consumption in various ways:</p>\n<p>CPU time used in user mode<br>\n    CPU time used in kernel mode<br>\n    Wall clock time</p>\n<p>For Isabelle (using JEdit), CPU time is consumed by Java (to run JEdit) and CPU time is consumed by<br>\nPoly/ML (to do inferences).  The CPU time spent by Java, while potentially significant, has to do with<br>\nthe manipulation of the annotated syntax tree of the theories being edited, depends primarily on the<br>\nsize of that document and its annotations (not on the complexity of proof steps) and is not of interest<br>\nto me here.  In any case, I assume that this time is not intended to be included in the figures reported<br>\nin the \"Timings\" panel and the values one sees if one CTRL-hovers over \"by\", \"apply\", etc.</p>\n<p>I would expect that the times shown in the \"Timings\" panel are intended to reflect the actual CPU time<br>\nspent by ML in user mode.  CPU time spent in user mode by ML may be partitioned into time running ML code<br>\nand time spent doing garbage collection.  It does not include time spent with the CPU idle waiting for paging,<br>\nor CPU time spent running other unrelated processes.  It probably also ought not to include CPU time spent<br>\nin kernel mode.  As the page fault rate of the ML process increases to the point of saturating the paging<br>\ndevices, the wall clock time taken to accomplish a particular task will increase dramatically.<br>\nThe CPU time spent in system mode will also increase, though this will typically not be that significant.<br>\nIn addition, once the heap size reaches its maximum value (as a result of actual hardware limitation or<br>\nartificial software limits placed on the ML process) the fraction of live data in the heap will become larger<br>\nand larger and consequently the garbage collector will consume more and more CPU time to perform the same<br>\nunderlying computational task.  However, as far as I know, the ML implementation accounts separately for<br>\nCPU time used in garbage collection versus CPU time used to run ML code, so it is not necessarily the<br>\ncase that garbage collection time should be a significant component of the \"Timings\" numbers.</p>\n<p>But you know all that.  Let me try again to make what I believe is a very simple point:  Empirical<br>\nobservation shows that figures reported under \"Timings\" <em>do not</em> reflect only time spent by ML running<br>\nML code in user mode: depending on the system conditions under which a task is run, they apparently also<br>\ninclude either CPU time spent in garbage collection, wall clock time spent waiting for paging,<br>\nor a combination of the two.  This is not desirable because it makes the timings not repeatable.<br>\nThe timings would be more useful if they would more or less reflect the amount of user CPU time spent<br>\nin user mode to run ML code to accomplish a particular computational task.  These numbers ought to agree<br>\nclosely with the amount of wall clock time it would take to accomplish that same task on a system in which<br>\nRAM size is large enough that there is an insignificant page fault rate, and on which there is no<br>\ncompetition for CPU with unrelated applications.  Under those conditions, one can expect the \"Timings\"<br>\nnumbers to be repeatable across separate runs, and that optimizations that reduce the numbers reported<br>\nunder \"Timings\" will reflect actual improvements in wall clock time when the task is run under conditions<br>\nwhere memory limitations are not significant.</p>\n<p>Well, that is the point I am trying to make.  Perhaps you are trying to say that one ought to be optimizing<br>\nmemory consumption rather than CPU consumption.  Well that would certainly be a valid point, but to do that<br>\nthat would require a \"Memory usage\" panel that reports the amount of allocation performed in conjunction with<br>\neach basic proof step.  I would expect this to be a more complex instrumentation task than recording CPU<br>\nusage associated with each basic proof step.  Since there is likely to be a tradeoff between memory utilization<br>\nand CPU consumption, it is not clear to me at the moment that this is primarily what one wants to do.</p>",
        "id": 208590877,
        "sender_full_name": "Email Gateway",
        "timestamp": 1598895527
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nThe Timing panel works with elapsed time. See also the general explanations in<br>\nsection 6.1 of the \"jedit\" manual (Documentation panel).</p>\n<p>If you want repeatable results for a single proof command, you can approximate<br>\nit like this:</p>\n<p>* Use sufficient physical memory to avoid use of virtual memory by the<br>\nunderlying OS (no disk thrashing while timing).</p>\n<p>* Avoid non-trivial applications running in parallel.</p>\n<p>* Disable Isabelle/ML multithreading by using system option threads=1 (e.g.<br>\nvia the Isabelle/jEdit menu Plugins / Plugin Options / Isabelle / General /<br>\nThreads.</p>\n<p>* Use a well-defined initial state of the ML heap to minimize subsequent<br>\ngarbage collection, e.g. via</p>\n<p>ML \"ML_Heap.share_common_data ()\"</p>\n<p>The latter is rather drastic: it is only required for substantial ML heap<br>\nusage, as in your own AFP/Bicategory<br>\n<a href=\"https://isabelle.sketis.net/devel/build_status/AFP_64bit_8_threads/index.html#session_Bicategory\">https://isabelle.sketis.net/devel/build_status/AFP_64bit_8_threads/index.html#session_Bicategory</a></p>\n<p>Makarius</p>",
        "id": 208670245,
        "sender_full_name": "Email Gateway",
        "timestamp": 1598957058
    }
]