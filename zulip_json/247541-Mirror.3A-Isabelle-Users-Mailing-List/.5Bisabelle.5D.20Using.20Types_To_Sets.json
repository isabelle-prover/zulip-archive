[
    {
        "content": "<p>From: Dominique Unruh &lt;<a href=\"mailto:cl-isabelle-users@lists.cam.ac.uk\">cl-isabelle-users@lists.cam.ac.uk</a>&gt;<br>\nHello,</p>\n<p>[tl;dr: I have worked with Types_To_Sets and am shortly reporting my <br>\nexperience and asking for feedback whether there are easier mechanisms <br>\nfor these kinds of tasks.]</p>\n<p>I have just used Types_To_Sets to translate a theorem stated on Hilbert <br>\nspaces on the whole type to one stated on a subspace on that type.</p>\n<p>Specifically, I'm translating this (the existence of an orthonormal basis):</p>\n<p>[source]<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Complex_Bounded_Operators/Complex_Inner_Product.thy#L1905\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Complex_Bounded_Operators/Complex_Inner_Product.thy#L1905</a>&gt;</p>\n<p>into this:</p>\n<p>[source]<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L758\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L758</a>&gt;</p>\n<p>It turns out that this is <em>a lot</em> of effort. I needed to do the <br>\nfollowing things before I could even start translating the theorem:</p>\n<p>* I had to make unoverloaded definitions//(e.g., /closure.with/<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L695\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L695</a>&gt;)<br>\n    //of many constants (those used in the theorem, and recursively<br>\n    those in the definitions). This is fortunately quite easy using the<br>\n    command <em>unoverload_definition</em>.</p>\n<p>* For each of those constants I had to define a related one \"on sets\".<br>\n    (E.g., /definition ‹nhds_on A open a = (⨅ (principal ` {S. S ⊆ A ∧<br>\n    open S ∧ a ∈ S})) ⊓ principal A›/<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L475\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L475</a>&gt;<br>\n    as an analogue to the constant nhds.)</p>\n<p>* For each of those constants I had to write a transfer-rule relating<br>\n    the one on sets to the original (unoverloaded) one. (E.g.,<br>\n    nhds_transfer<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L478\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L478</a>&gt;.)<br>\n    Interestingly, I had in various cases to unfold definitions using<br>\n    the definitional axiom (e.g.,<br>\n    Topological_Spaces.topological_space.nhds_def_raw<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L484\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L484</a>&gt;)<br>\n    because the unoverloaded ends up using \"internal\" constants such as<br>\n    topological_space.nhds instead of nhds.</p>\n<p>* Those constants on sets, I needed to relate to existing definitions<br>\n    on sets. (E.g., /‹nhds_on (topspace T) (openin T) x = nhdsin T x› if<br>\n    ‹x ∈ topspace T›/<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L493\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L493</a>&gt;<br>\n    to related the nonstandard /nhds_on /to the existing /nhdsin/ on the<br>\n    type /topology/. (I cannot directly write transfer-rules from the<br>\n    unoverloaded constants to those existing definitions on sets because<br>\n    the transfer mechanism leads to definitions that behave different<br>\n    from the builtin ones in \"bad\" situation, e.g., when a function maps<br>\n    to values outside its supposed range.) In some cases, the<br>\n    definitions don't match up nicely, and one needs to do some<br>\n    non-trivial proofs to relate them (i.e., actually the math, not just<br>\n    translating terms, e.g. here<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L202\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L202</a>&gt;).</p>\n<p>* Additionally, I had to (again recursively) write transfer theorems<br>\n    and \"definitions on sets\" for the type class /chilbert_space/<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L617\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L617</a>&gt;<br>\n    and all its ancestors.</p>\n<p>* And I had to relate those definitions on sets to existing ones again<br>\n    (e.g., ‹chilbert_space_on V (<em>⇩R) (</em>⇩C) (+) 0 (-) uminus dist norm<br>\n    sgn (uniformity_on V) (openin (top_of_set V)) (∙⇩C)› if ‹complete V›<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L637\">https://github.com/dominique-unruh/afp/blob/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy#L637</a>&gt;).</p>\n<p>All of this is a lot of effort. In fact, the Isabelle code just for <br>\ntranslating the theorem is over 800 lines (available here <br>\n&lt;<a href=\"https://raw.githubusercontent.com/dominique-unruh/afp/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy\">https://raw.githubusercontent.com/dominique-unruh/afp/5ceb065bd5f9f4469717a11a73d894b0711b44f0/thys/Registers/Tmp_Move.thy</a>&gt;, <br>\nneeds Isabelle2022 with AFP). Of these, about 750 are just setup.</p>\n<p>So I am wondering: Are there better ways to do this? Are there <br>\nautomations for this that I am not aware of? Other shortcuts? Etc.?</p>\n<p>(And if not, then I leave this out here as an instructive example how to <br>\ndo a nontrivial transfer with Types_To_Sets.)</p>\n<p>Best wishes,<br>\nDominique.<br>\n<a href=\"/user_uploads/14278/xE61jX8QiPzLIgECZ2BfEGSX/DOd7tJAoTaqHiL0u.png\">DOd7tJAoTaqHiL0u.png</a><br>\n<a href=\"/user_uploads/14278/qc6-k6A5P84oIFhkkRfDNl-6/ymc1JRSESGOFkrkG.png\">ymc1JRSESGOFkrkG.png</a></p>",
        "id": 310479416,
        "sender_full_name": "Email Gateway",
        "timestamp": 1668625682
    },
    {
        "content": "<p>From: Andrei Popescu &lt;<a href=\"mailto:andrei.h.popescu@gmail.com\">andrei.h.popescu@gmail.com</a>&gt;<br>\nHi Dominique,</p>\n<p>Mihails Milehins has a tool in the AFP that automates much of the process:<br>\n<a href=\"https://www.isa-afp.org/entries/Types_To_Sets_Extension.html\">https://www.isa-afp.org/entries/Types_To_Sets_Extension.html</a><br>\nHe also describes this in his CPP paper:<br>\n<a href=\"https://gitlab.com/user9716869/etts_preprint/-/raw/master/ETTS_Preprint.pdf\">https://gitlab.com/user9716869/etts_preprint/-/raw/master/ETTS_Preprint.pdf</a></p>\n<p>Best wishes,<br>\nAndrei</p>",
        "id": 310483008,
        "sender_full_name": "Email Gateway",
        "timestamp": 1668626805
    },
    {
        "content": "<p>From: Andrei Popescu &lt;<a href=\"mailto:andrei.h.popescu@gmail.com\">andrei.h.popescu@gmail.com</a>&gt;<br>\nAnd just to advertise some further developments on the topic: In<br>\nrecent work (with Dmitriy Traytel)<br>\n<a href=\"https://www.andreipopescu.uk/pdf/types2pers_POPL2023.pdf\">https://www.andreipopescu.uk/pdf/types2pers_POPL2023.pdf</a><br>\nwe perform truly pervasive types-to-sets relativization without<br>\nstructural restrictions, in the more general form of \"types to PERs\".<br>\nThe tool linked from the paper is currently in a very prototypical<br>\nstage, and adds axioms -- even though we show meta-theoretically that<br>\nthe axioms can be replaced by proved statements.</p>\n<p>Best wishes,<br>\nAndrei</p>",
        "id": 310486335,
        "sender_full_name": "Email Gateway",
        "timestamp": 1668627773
    },
    {
        "content": "<p>From: Dominique Unruh &lt;<a href=\"mailto:cl-isabelle-users@lists.cam.ac.uk\">cl-isabelle-users@lists.cam.ac.uk</a>&gt;<br>\nHi,</p>\n<p>thanks for the pointer. I tried out ETTS (i.e., tried porting my code to <br>\nuse it instead). Here are my experiences (and further thoughts). Note <br>\nthat these are experiences after a smaller project (1000 loc) so may not <br>\nor may not be accurate.</p>\n<p>* ETTS (and the connected project CTR) provide automation for defining<br>\n    unoverloaded and set-based constants and for transferring type-based<br>\n    theorems to set-based theorems. If everything works smoothly, that<br>\n    means that one can transfer both constants and theorems with minimal<br>\n    boilerplate.</p>\n<p>* I feel that in practice, it is rare that everything works smoothly.<br>\n    In many cases, one simply gets an answer such as \"couldn't find a<br>\n    transfer rule\". This may be due to either the need to first generate<br>\n    transfer-rules for other dependent constants, or simply because the<br>\n    terms in questions are not good for transferring. (E.g., contain<br>\n    constants that do not have admit strong parametricity- or<br>\n    transfer-rules.)</p>\n<p>* These problems also occur when directly writing transfer theorems<br>\n    but it is easier to debug. (E.g., by replacing /apply<br>\n    transfer_prover /by /apply transfer_prover_start apply<br>\n    transfer_step+/.) So I found myself often proving the required<br>\n    transfer theorems by hand (and first defining the unoverloaded<br>\n    constants by hand), and then replacing the result by a CTR call.</p>\n<p>* An additional difficulty is that to understand how to use (and<br>\n    tweak) CTR/ETTS, one needs to understand the TTS and transfer<br>\n    mechanisms quite well. So ETTS does not help users who cannot use<br>\n    TTS manually anyway. (ETTS may lead to shorter code but the effort<br>\n    to produce this code does not seem smaller to me.)</p>\n<p>* I did not get the tts_lemmas command itself to work. (But I did not<br>\n    try long after getting an error because the number of theorems that<br>\n    I wanted to transfer is quite low so I stuck to the manual approach<br>\n    there.)</p>\n<p>* In the end, my code length actually increased to my surprise. (Point<br>\n    to note: I transferred classes involving filters such as<br>\n    metric_space etc. Filters are quite fiddlesome when trying to do<br>\n    transfers because rel_filter is a complicated beast.)</p>\n<p>Altogether, I would say there there is still a lot of research potential <br>\nto make TTS more userfriendly. (But ETTS is already an important and <br>\nimpressive step. It's just that TTS in its nature is complicated.)</p>\n<p>In the end, I came to follow the following approach for my own project <br>\n(maybe this guidelines is of use for others):</p>\n<p>* For each involved class X, do the following:<br>\n      o Define a locale /class_X_ow /with one additional parameter U<br>\n        (the carrier set) but otherwise analogous to the definition of<br>\n        the predicate /class.X/. (E.g., metric_space_ow<br>\n        &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L638\">https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L638</a>&gt;.)<br>\n        Try to write the definitions already so that parametricity<br>\n        proofs are easy. In most cases, this is done by simply replacing<br>\n        unbounded \"∀x\" by \"∀x∈U\" but in some cases a little more care is<br>\n        needed.<br>\n      o Prove a parametricity result /lemma class_X_ow_parametricity<br>\n        assumes \"bi_unique T\" shows \"(relator involving T)<br>\n        //X_ow_parametricity //X_ow_parametricity\"/. (Here I deviate<br>\n        from the approach used by ETTS. There one would prove a<br>\n        transfer-rule relating the unoverloaded definition with the<br>\n        set-based definition and with assumptions /bi_unique T/ and<br>\n        /right_total T/. My approach avoids the definition of the<br>\n        intermediate unoverloaded but type-based constant. Sometimes the<br>\n        /ctr parametricity/ command manages this. (E.g., here &lt;ctr<br>\n        parametricity in metric_space_ow_def[unfolded<br>\n        metric_space_ow_axioms_def]&gt; or here<br>\n        &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L517\">https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L517</a>&gt;.)<br>\n      o Prove an unoverloading result of the form: /lemma class_ud:<br>\n        \"class.X = class_X_ow UNIV\". /In most cases, this works quite<br>\n        easily by unfolding the definitions and using /simp/. E.g. here<br>\n        &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L645\">https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L645</a>&gt;.<br>\n      o Note that this approach needs less definitions than the ETTS<br>\n        approach because we do not define first an unoverloaded version<br>\n        and then a transferred version. We still get everything that we<br>\n        have in ETTS: /class_//X_ow UNIV/ is the same as the<br>\n        unoverloaded (but not yet set-based) constant.<br>\n      o Prove various helping lemmas for standard cases, such as<br>\n        /class_metric_space_ow_typeclass/<br>\n        &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L648&gt;that\">https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L648&gt;that</a><br>\n        servers as an introduction rule for the _ow predicate when we do<br>\n        have use the type-class /metric_space///on the set-based side.</p>\n<p>* For each involved constant X, do the following:<br>\n      o Define /X_ow/, the unoverloaded set-based constant (e.g.,<br>\n        nhds_ow<br>\n        &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L656\">https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L656</a>&gt;).<br>\n      o Prove parametricity (e.g. here &lt;ctr parametricity in<br>\n        nhds_ow_def[folded transfer_bounded_filter_Inf_def, unfolded<br>\n        make_parametricity_proof_friendly]&gt;) and an unoverloading result<br>\n        (e.g., here<br>\n        &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L664\">https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L664</a>&gt;)<br>\n        like with the class. And auxiliary lemmas (e.g., here<br>\n        &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L667\">https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L667</a>&gt;<br>\n        one relating /nhds_ow /to the existing /nhds//_in/ based on the<br>\n        type /'a topology/.)</p>\n<p>*   Finally, all this can be used for transferring theorems, I give<br>\n    one example here<br>\n    &lt;<a href=\"https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L959\">https://github.com/dominique-unruh/afp/blob/ba7d9eb8dbe6e877c195c8077b65c6fae9d792cc/thys/Registers/Tmp_Move.thy#L959</a>&gt;.</p>\n<p>Note the difference in approach: we do not define two unoverloaded <br>\nconstants but one. As a consequence, only one transfer-theorem is used <br>\nper constant. Also, in my opinion, things become more regular this way: <br>\nA type-based constant can often be defined in numerous equivalent ways. <br>\nBut once unoverloaded, the definitions become non-equal. And the ones <br>\nthat arise from unoverloading directly may then not be suitable for a <br>\ntransfer theorem (because the transfer theorem can then not assume that <br>\nthe laws of the original type-class are satisfied). (E.g., the sum over <br>\nan abelian monoid can choose any order in which to add, or even require <br>\nby definition that all the ways of adding give the same result. But when <br>\nI underload sum, I get something that invole THE or SOME which is very <br>\nunsuitable for transferring.) My approach forces imposes some discpline <br>\nthat avoids falling into this trap. Whether it is preferrable for <br>\ntransferring large libraries I cannot tell but I thought I put it out <br>\nthere for inspiration.</p>\n<p>Best wishes,<br>\nDominique.</p>",
        "id": 311420972,
        "sender_full_name": "Email Gateway",
        "timestamp": 1669048497
    },
    {
        "content": "<p>From: Mihails Milehins &lt;<a href=\"mailto:mihailsmilehins@gmail.com\">mihailsmilehins@gmail.com</a>&gt;<br>\nDear Dominique Unruh,</p>\n<p>Thank you for your feedback about the ETTS. I would like to make several<br>\nremarks with regard to your feedback:</p>\n<ol>\n<li>\n<p>The UD/CTR provide auxiliary/complementary commands. Neither one is a<br>\nprerequisite for using the ETTS. Neither one of them is universally<br>\napplicable. They merely implement the algorithms that are described in the<br>\nuser manuals, hopefully, faithfully. Indeed, your assumption is correct,<br>\nneither creates the constants/transfer rules recursively. Thus, for<br>\nexample, for the operation of the CTR, the constants that occur as subterms<br>\nin the definitions need to be relativized explicitly. Also, the CTR suffers<br>\nfrom the same fundamental limitations that the algorithm used in the<br>\ntransfer_prover suffers from, as the CTR merely provides an interface to<br>\nit. However, most of this is already stated in the manuals/paper or, at<br>\nleast, can be easily inferred from the descriptions of the algorithms.</p>\n</li>\n<li>\n<p>The ETTS was built around an implementation of a variant of the<br>\ntypes-to-sets algorithm that is described in the manual/paper. The<br>\nimplementation is available via the command tts_lemmas. This algorithm is<br>\nlargely based on the algorithm from [1], with some additional \"features\"<br>\nbased on [2]. In a sense, the ETTS is meant to be applied in a manner<br>\nsimilar to how one would use traditional classical reasoners/simp-based<br>\nmethods (like auto/force) with additional settings (trial and error) and,<br>\nin my view, provides a similar level of automation over proving each step<br>\nmanually using the rule-based tactics. Indeed, there are times when I feel<br>\nlike it is easier to use rule-based tactics explicitly, than making<br>\nauto/force work implicitly. A good rhetorical question to ask here would<br>\nbe: are auto/force tools inferior in comparison to rule application or am<br>\nI a \"bad\" user of auto/force? Perhaps, the answer is neither. With<br>\nexperience we all learn when methods like auto/force are more suitable, and<br>\nwhen the preference should be given to direct rule application. However,<br>\nlike with the types-to-sets-related-technologies, there is much scope for<br>\nthe improvement of traditional proof methods (the only difference being<br>\nthat types-to-sets is still quite novel and specialized, whereas the<br>\ntraditional proof methods and their implementations have been refined over<br>\ndecades by a large number of users/developers).</p>\n</li>\n<li>\n<p>The most common problem for the operation of the ETTS is the<br>\nunavailability of suitably stated transfer rules. It has an in-built \"proof<br>\ndebugger\" that can be invoked by adding \"!\" to the command tts_lemmas. This<br>\nshows the steps invoked by the algorithm, and it is meant to be used<br>\nextensively during the \"setup\" phase of the ETTS, while looking for<br>\nsuitable transfer rules for the constants associated with a given theory.<br>\nThe full unconstrained automation of the synthesis of arbitrary transfer<br>\nrules was never claimed anywhere in the paper/manual, merely the automation<br>\nof the application of a variant of a types-to-sets algorithm, if the<br>\nprerequisites for its application are satisfied.</p>\n</li>\n<li>\n<p>I have to admit, subjectively your feedback feels quite similar to the<br>\nfeedback that new (hypothetical) users of Isabelle often give to the method<br>\n\"auto\" on Stack Overflow when trying it for the first time: \"it just gives<br>\nme an error (failed to finish proof) and it does not prove the theorems<br>\nthat I want!\". I was expecting a more in-depth feedback from experienced<br>\nusers of Isabelle before applying a certain amount of smear to someone<br>\nelse’s work :-). However, of course, words are cheap. As you can see from<br>\nthe ETTS AFP entry, I performed the relativization of significant parts of<br>\nthe main library using the ETTS, including essentially all of the<br>\nrelativization work performed by Fabian Immler and Bohua Zhan in [2].<br>\nFurthermore, thus far, I only reported on a fraction of all of the theorems<br>\nthat I relativized while trying the tool. For example, I performed the<br>\nrelativization of almost all of the theorems about topological spaces,<br>\nincluding the ones that required filters (the latter, indeed, was not<br>\ntrivial and required some additional ad-hoc manual proof effort related to<br>\nthe transfer infrastructure). Unfortunately, these efforts are now lost,<br>\nbut I can recreate them within a reasonable time frame.</p>\n</li>\n<li>\n<p>I would be happy to take on the challenge of performing the<br>\nrelativization of the theorems that you would like to relativize using the<br>\nETTS in my spare time. Feel free to point me to the specific theories for<br>\nwhich you would like the relativization to be performed using the ETTS.<br>\nHowever, presently, I have little time to dedicate to such matters (the<br>\ntime around Christmas would probably be the best time for me). Therefore, I<br>\ncannot guarantee a very quick response time.</p>\n</li>\n</ol>\n<p>Yours Sincerely,<br>\nMihails Milehins<br>\n(he/him/his)</p>\n<ol>\n<li>\n<p>Kunčar O, Popescu A. From Types to Sets by Local Type Definition in<br>\nHigher-Order Logic. Journal of Automated Reasoning. 2019;62(2):237–60.</p>\n</li>\n<li>\n<p>Immler F, Zhan B. Smooth Manifolds and Types to Sets for Linear Algebra<br>\nin Isabelle/HOL. In: Mahboubi A, Myreen MO, editors. Proceedings of the 8th<br>\nACM SIGPLAN International Conference on Certified Programs and Proofs, CPP<br>\n2019, Cascais, Portugal. New York, NY, USA: ACM; 2019. p. 65–77. (CPP<br>\n2019).</p>\n</li>\n</ol>",
        "id": 312214390,
        "sender_full_name": "Email Gateway",
        "timestamp": 1669391070
    },
    {
        "content": "<p>From: Dominique Unruh &lt;<a href=\"mailto:cl-isabelle-users@lists.cam.ac.uk\">cl-isabelle-users@lists.cam.ac.uk</a>&gt;<br>\nHi,</p>\n<p>sorry for the late answer...</p>",
        "id": 315977404,
        "sender_full_name": "Email Gateway",
        "timestamp": 1671091488
    },
    {
        "content": "<p>From: Mihails Milehins &lt;<a href=\"mailto:mihailsmilehins@gmail.com\">mihailsmilehins@gmail.com</a>&gt;<br>\nDear Dominique Unruh/All,</p>\n<p>Thank you for your email.</p>\n<p>Since I already lifted the theorems using my own approach at the time of</p>\n<blockquote>\n<p>writing, I do not necessarily <em>need</em> a relativization using ETTS for my<br>\nuse-case. But if you think it would be interesting as a case study, I would<br>\nat least be curious to see how easy or hard it is to do with ETTS.</p>\n<p>...<br>\nBut as I said – only if you find this interesting yourself. I will<br>\nprobably not include it in my theories because I already have a solution.</p>\n</blockquote>\n<blockquote>\n<p>If I understood correctly, there were three theorems for which the<br>\nrelativization needed to be performed:</p>\n</blockquote>\n<p>1. on_closure_eqI<br>\n   2. orthonormal_basis_exists<br>\n   3. has_sum_comm_additive_general</p>\n<p>I felt that it could be beneficial for me to demonstrate that the ETTS can<br>\nbe used for the relativization of the aforementioned theorems, because your<br>\nprevious comments did cast doubt on the applicability of the ETTS to these<br>\ntheorems. However, given that the results are not needed for any practical<br>\npurpose and (as I explain below) I have little personal interest in the<br>\nETTS, I provide only the relativization of the theorems on_closure_eqI and<br>\nhas_sum_comm_additive_general (<br>\n<a href=\"https://gitlab.com/user9716869/etts_sml_extra/\">https://gitlab.com/user9716869/etts_sml_extra/</a> - this is meant to be used<br>\nwith Isabelle2022 and the associated release version of the AFP). This was<br>\nachieved without any alterations of the ETTS or the methodology for its<br>\napplication proposed in the SML examples suite. I hope that it is apparent<br>\nto see that orthonormal_basis_exists can also be lifted using the ETTS in a<br>\nsimilar manner with some further effort on the side of the relativization<br>\nof constants.</p>\n<p>I will use this opportunity to make several general remarks:</p>\n<p>1. As I mentioned before, I believe that it can be easy to conflate<br>\n   several interrelated topics when assessing the UD/CTR/ETTS. The UD/CTR/ETTS<br>\n   provide an implementation/automation/infrastructure for the application of<br>\n   several algorithms that were suggested in the published literature (e.g.,<br>\n   [2] and [3]). The relativization algorithm implemented as part of the ETTS<br>\n   requires every constant that occurs in the input theorem to be relativized<br>\n   (in a predefined format) prior to its application. AFAIK, no algorithm for<br>\n   fully automated relativization of an arbitrary constant in Isabelle/HOL was<br>\n   proposed before/while I was working on the ETTS. While the frameworks<br>\n   UD/CTR/parametricity provide a partial solution for the relativization of<br>\n   constants, it is expected that some manual effort will also be (inevitably)<br>\n   required. Essentially all of the manual proofs in<br>\n<a href=\"https://gitlab.com/user9716869/etts_sml_extra/\">https://gitlab.com/user9716869/etts_sml_extra/</a> are somehow related to<br>\n   the relativization of constants, not the application of the relativization<br>\n   algorithm. An entirely different topic is the presentation of the<br>\n   relativized results in a specific format desired by the user. Here, the<br>\n   ETTS provides a range of utilities up to the application of certain<br>\n   attributes for post-processing of the results of the relativization at a<br>\n   large scale. Nonetheless, it is difficult to disagree with your original<br>\n   criticism that there is some scope for the improvement of the ETTS/UD/CTR:<br>\n   a common saying in commercial software development is \"software can only be<br>\n   released, never finished\". My own backlog for the ETTS was growing<br>\n   continuously throughout its development, and its release was based on the<br>\n   time I was happy to dedicate to its development, rather than any other<br>\n   factor.</p>\n<p>2. <a href=\"https://gitlab.com/user9716869/etts_sml_extra/\">https://gitlab.com/user9716869/etts_sml_extra/</a> provides many<br>\n   additional results, not directly related to the theorems in<br>\n<a href=\"https://github.com/dominique-unruh/afp/blob/306ef85bea71cea76b5794c76ef260ab1ffa46ff/thys/Hilbert_Space_Tensor_Product/Misc_Tensor_Product_TTS.thy#L944\">https://github.com/dominique-unruh/afp/blob/306ef85bea71cea76b5794c76ef260ab1ffa46ff/thys/Hilbert_Space_Tensor_Product/Misc_Tensor_Product_TTS.thy#L944</a>.<br>\n   These results are not needed for the relativization of the theorems in<br>\n   question. However, their creation required almost no extra effort, and I<br>\n   thought that it does not hurt to provide them given the opportunity.</p>\n<p>3. Overall, I largely abandoned my work on the ETTS in 2019 due to the<br>\n   lack of personal interest in this subject. Nonetheless, I felt a<br>\n   necessity to refine this work to the extent that it could be considered<br>\n   close to production level and make an attempt to publish it because much of<br>\n   the community's time was invested in it (in fact, I allowed this<br>\n   development to be largely driven by the desires of the community, providing<br>\n   merely an implementation of the ideas suggested to me) and because I<br>\n   was encouraged to do so by several members of the community. Therefore, I<br>\n   was very pleased to find out that this line of work is being taken in a<br>\n   different direction [1], as I felt that it would be a chore for me to<br>\n   respond to feedback about the ETTS or improve the ETTS in any way (I became<br>\n   aware of the other line of work some time before the publication of my own<br>\n   work at the CPP). For these two reasons, I also minimized the effort on<br>\n   investigating the current issue, and only provided a sketch of a solution<br>\n   using the ETTS, not a refined production level code immediately suitable<br>\n   for your specific application.</p>\n<p>4. Finally, from a personal perspective, I do not recommend the use of<br>\n   the original framework types-to-sets (not only the ETTS) for the<br>\n   formalization of mathematics at large. The reasons for this are largely<br>\n   subjective, but go beyond the use of the additional axioms. Despite that I<br>\n   tried not to criticize publicly my own work on types-to-sets and its<br>\n   predecessors prior to the publication of the ETTS as a courtesy to those<br>\n   who invested time in helping me, I became a staunch opponent of the library<br>\n   design patterns that rely on the conversion of type-based-results to any<br>\n   form of set-based-results (or vice versa, or any other form of duplication)<br>\n   towards the end of 2019. I believe that maintaining entire \"duplicate<br>\n   libraries\" of constants and results is exceptionally inefficient: the<br>\n   priority in the area of library design should be the elimination of<br>\n   duplication, not finding ways to generate more \"semantic duplicates\"<br>\n   automatically (of course, this is not an attempt to criticize the transfer<br>\n   tools at large, merely their use for the production of<br>\n   \"semantic duplicates\" in the context of the design of libraries of<br>\n   formalized mathematics).</p>\n<p>Lastly, Merry Christmas and a Happy New Year!</p>\n<p>Yours Sincerely,<br>\nMihails Milehins<br>\n(he/him/his)</p>\n<ol>\n<li>\n<p>Popescu A, Traytel D. Admissible Types-To-PERs Relativization in<br>\nHigher-Order Logic. POPL 2023 (to appear).</p>\n</li>\n<li>\n<p>Kunčar O, Popescu A. From Types to Sets by Local Type Definition in<br>\nHigher-Order Logic. Journal of Automated Reasoning. 2019;62(2):237–60.</p>\n</li>\n<li>\n<p>Immler F, Zhan B. Smooth Manifolds and Types to Sets for Linear Algebra<br>\nin Isabelle/HOL. In: Mahboubi A, Myreen MO, editors. Proceedings of the 8th<br>\nACM SIGPLAN International Conference on Certified Programs and Proofs, CPP<br>\n2019, Cascais, Portugal. New York, NY, USA: ACM; 2019. p. 65–77. (CPP<br>\n2019).</p>\n</li>\n</ol>\n<p>On Thu, Dec 15, 2022 at 10:03 AM Dominique Unruh &lt;<a href=\"mailto:unruh@ut.ee\">unruh@ut.ee</a>&gt; wrote:</p>\n<blockquote>\n<p>Hi,</p>\n<p>sorry for the late answer...</p>\n<ol start=\"5\">\n<li>I would be happy to take on the challenge of performing the<br>\nrelativization of the theorems that you would like to relativize using the<br>\nETTS in my spare time. Feel free to point me to the specific theories for<br>\nwhich you would like the relativization to be performed using the ETTS.<br>\nHowever, presently, I have little time to dedicate to such matters (the<br>\ntime around Christmas would probably be the best time for me). Therefore, I<br>\ncannot guarantee a very quick response time.</li>\n</ol>\n<p>Since I already lifted the theorems using my own approach at the time of<br>\nwriting, I do not necessarily <em>need</em> a relativization using ETTS for my<br>\nuse-case. But if you think it would be interesting as a case study, I would<br>\nat least be curious to see how easy or hard it is to do with ETTS.</p>\n<p>You can find the relativizations that I was proving here:<br>\n<a href=\"https://github.com/dominique-unruh/afp/blob/306ef85bea71cea76b5794c76ef260ab1ffa46ff/thys/Hilbert_Space_Tensor_Product/Misc_Tensor_Product_TTS.thy#L944\">https://github.com/dominique-unruh/afp/blob/306ef85bea71cea76b5794c76ef260ab1ffa46ff/thys/Hilbert_Space_Tensor_Product/Misc_Tensor_Product_TTS.thy#L944</a><br>\nstarting from line 944.</p>\n<p>But as I said – only if you find this interesting yourself. I will<br>\nprobably not include it in my theories because I already have a solution.</p>\n<p>Best wishes,<br>\nDominique.</p>\n<p>On 2022-11-22 5:19, Mihails Milehins wrote:</p>\n<p>Dear Dominique Unruh,</p>\n<p>Thank you for your feedback about the ETTS. I would like to make several<br>\nremarks with regard to your feedback:</p>\n<ol>\n<li>\n<p>The UD/CTR provide auxiliary/complementary commands. Neither one is a<br>\nprerequisite for using the ETTS. Neither one of them is universally<br>\napplicable. They merely implement the algorithms that are described in the<br>\nuser manuals, hopefully, faithfully. Indeed, your assumption is correct,<br>\nneither creates the constants/transfer rules recursively. Thus, for<br>\nexample, for the operation of the CTR, the constants that occur as subterms<br>\nin the definitions need to be relativized explicitly. Also, the CTR suffers<br>\nfrom the same fundamental limitations that the algorithm used in the<br>\ntransfer_prover suffers from, as the CTR merely provides an interface to<br>\nit. However, most of this is already stated in the manuals/paper or, at<br>\nleast, can be easily inferred from the descriptions of the algorithms.</p>\n</li>\n<li>\n<p>The ETTS was built around an implementation of a variant of the<br>\ntypes-to-sets algorithm that is described in the manual/paper. The<br>\nimplementation is available via the command tts_lemmas. This algorithm is<br>\nlargely based on the algorithm from [1], with some additional \"features\"<br>\nbased on [2]. In a sense, the ETTS is meant to be applied in a manner<br>\nsimilar to how one would use traditional classical reasoners/simp-based<br>\nmethods (like auto/force) with additional settings (trial and error) and,<br>\nin my view, provides a similar level of automati<br>\n[message truncated]</p>\n</li>\n</ol>\n</blockquote>",
        "id": 317839927,
        "sender_full_name": "Email Gateway",
        "timestamp": 1671976925
    },
    {
        "content": "<p>From: Dominique Unruh &lt;<a href=\"mailto:cl-isabelle-users@lists.cam.ac.uk\">cl-isabelle-users@lists.cam.ac.uk</a>&gt;<br>\nDear Mihails Milehins,</p>\n<p>thanks for the example theories. It is definitely interesting to see how <br>\ndifferent approaches compare on this problem.</p>\n<blockquote>\n<p>I believe that maintaining entire \"duplicate libraries\" of constants <br>\nand results is exceptionally inefficient: the priority in the area of <br>\nlibrary design should be the elimination of duplication, not finding <br>\nways to generate more \"semantic duplicates\" automatically (of course, <br>\nthis is not an attempt to criticize the transfer tools at large, merely <br>\ntheir use for the production of \"semantic duplicates\" in the context of <br>\nthe design of libraries of formalized mathematics).</p>\n</blockquote>\n<p>I do agree that having libraries of duplicates is probably undesirable, <br>\nand that the TTS approach (at least at its current level of maturity) <br>\nleads to a lot of boilerplate and duplication. However, I fail to see an <br>\nalternative to the occasional use of it. For example, the three theorems <br>\nI needed to convert were not converted simply out of a which to <br>\ngeneralize the library. Instead, I needed those theorems in a subproof <br>\nsomewhere in further developments in their set-based form (a certain <br>\nalready proven lemma needed to be applied to a subset of UNIV), and I do <br>\nnot see any way to avoid the TTS conversion in such a use-case. (The <br>\nalternative, redoing the proof of the already proven lemma would seem <br>\nmuch worse, especially since that lemma uses a lot of other lemmas on <br>\nthe way, and I would have to reprove large parts of existing <br>\ndevelopments for that.)</p>\n<p>Therefore my approach is to develop libraries in a type-based <br>\npresentation, but keeping the option open to do a TTS translation in the <br>\nrare cases where this is needed. But I am open to thoughts about <br>\nalternatives to this approach. (From the Isabelle community at large, <br>\nnot just from Mihails.)</p>\n<p>Best wishes,<br>\nDominique.</p>",
        "id": 318014836,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672096606
    },
    {
        "content": "<p>From: Andrei Popescu &lt;<a href=\"mailto:andrei.h.popescu@gmail.com\">andrei.h.popescu@gmail.com</a>&gt;<br>\nHi Dominique and Mihails,</p>\n<blockquote>\n<p>Therefore my approach is to develop libraries in a type-based presentation, but keeping the option open to do a TTS translation in the rare cases where this is needed. But I am open to thoughts about alternatives to this approach. (From the Isabelle community at large, not just from Mihails.)</p>\n</blockquote>\n<p>In the case of libraries that have already been developed type-based,<br>\nI also do not see an alternative different from the two that were<br>\nmentioned: (1) restating the results and redoing the proofs set-based<br>\nor (2) TTS. And yes, this is exactly how we originally envisioned TTS:<br>\ntranslate only when needed.</p>\n<p>Moreover, there is also a case to be made for TTS for libraries that<br>\nare yet to be developed. While a set-based development would avoid the<br>\nduplication mentioned by Mihails, a type-based development is of<br>\ncourse easier -- which can make a big difference in productivity. Not<br>\nto mention that for large enough libraries, the duplication introduced<br>\nby (E)TTS might be dwarfed by the reduction in the overall size of the<br>\nlemmas and proofs.</p>\n<p>Best wishes,<br>\nAndrei</p>",
        "id": 318017208,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672099058
    },
    {
        "content": "<p>From: Mihails Milehins &lt;<a href=\"mailto:mihailsmilehins@gmail.com\">mihailsmilehins@gmail.com</a>&gt;<br>\nDear All,</p>\n<p>Thank you for your replies.</p>",
        "id": 318146482,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672168422
    },
    {
        "content": "<p>From: Andrei Popescu &lt;<a href=\"mailto:andrei.h.popescu@gmail.com\">andrei.h.popescu@gmail.com</a>&gt;<br>\nDear Mihails,</p>\n<p>Doubting the need for set-based developments leads to a useful<br>\ndiscussion, which is important IMO. I have not done a systematic<br>\nempirical study, but based on what I've seen I believe such<br>\ndevelopments are common in Isabelle/HOL (even beyond what is in the<br>\nlibrary), and perhaps in the other HOL-based provers as well. The<br>\npractice seems mostly motivated by the desire to instantiate the<br>\nresults flexibly. Otherwise at instantiation time one pays the price<br>\nof having to define some types and transfer some structure.</p>\n<p>My favourite simple example is the following: For instantiating<br>\nresults about groups to the set of bijections between 'a and 'a, you<br>\nmay not want to define a custom type -- especially if the results<br>\nabout bijections are used as part of a bigger whole, involving<br>\narbitrary functions as well (employing results that may  instantiate<br>\nother libraries as well).</p>\n<p>Another example: Say you develop some general results about labelled<br>\ntransition systems and their induced traces. Working with the<br>\ncollections of labels, events, states etc. in a type-based fashion<br>\n(i.e., having entire types to model these collections) will be painful<br>\nwhen instantiating the theory to the semantics of particular<br>\nprogramming languages, where any well-formedness conditions would need<br>\nto be baked into the type.</p>\n<p>In conclusion, I don't think that set-based results are a legacy issue<br>\nthat we should aim to replace.</p>\n<p>As for your remark that the type-based version of a theorem is<br>\nessentially semantically equivalent to its set-based counterpart: I<br>\nagree this is the case, just that the HOL logic cannot \"see'' this --<br>\nwhich is why in our initial paper we advocated the addition of the<br>\nLocal Typedef rule as \"a gentle eye surgery\" to HOL. Our last result<br>\n(which you also cited in your message) shows that this rule is in fact<br>\n(usually) admissible. So the set-based and type-based versions are<br>\nindeed very close, but still converting one to the other needs an<br>\nextra push.</p>\n<p>Finally, I do not have anything to add concerning the set theory<br>\nversus type theory debate. Everything that could have been said, and<br>\neven more, has probably been said over the years. :-) I can see the<br>\nrelative advantages of both, and I think HOL is somewhere in the<br>\nmiddle -- a sweet spot perhaps, with some touches of bitterness.</p>\n<p>Best wishes,<br>\nAndrei</p>",
        "id": 318155899,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672173076
    },
    {
        "content": "<p>From: Mihails Milehins &lt;<a href=\"mailto:mihailsmilehins@gmail.com\">mihailsmilehins@gmail.com</a>&gt;<br>\nDear Andrei Popescu/All,</p>\n<p>Thank you for your comments. I will try to provide concluding remarks from<br>\nmy side. It seems that there are 3 possible evolution paths for<br>\nIsabelle/HOL core libraries:</p>\n<ol>\n<li>\n<p>Both type-based and set-based libraries will continue to be maintained<br>\nand developed in parallel, now with the aid of tools like<br>\ntypes-to-sets/types-to-PERs.</p>\n</li>\n<li>\n<p>Migration to a single type-based library.</p>\n</li>\n<li>Migration to a single set-based library.<br>\nI doubt that it will be possible to \"prove\" objectively that one path<br>\nshould be preferred over another. However, having tried all of them, even<br>\nwith the aid of the state-of-the-art types-to-sets, my personal preference<br>\nwould be 2 or 3 (despite having to define additional types in 2 or the<br>\ninefficiencies associated with 3). Therefore, I guess, my initial comment<br>\nthat started this debate was prompted primarily by my concern about the<br>\nstructure of the core library (and its extensions), rather than tools like<br>\ntypes-to-sets. Indeed, if 1 is to be followed, having tools like<br>\ntypes-to-sets and its successor(s) is better than not having them.<br>\nNonetheless, I still hope that one day, somehow, these distinct libraries<br>\nwill coalesce into a homogeneous whole, with a single definition for each<br>\nstructure/concept, one statement and one proof of each result.</li>\n</ol>\n<p>Also, I agree that much has been said already about the use of<br>\nalternative foundations and there is no reason to return to this topic. My<br>\ninitial remark was prompted by my observation that the duplication problem<br>\nseems to be significantly easier to avoid in some of these alternative<br>\nsystems. So, I hope that it was not entirely off-topic.</p>\n<p>Yours Sincerely,<br>\nMihails Milehins<br>\n(he/him/his)</p>",
        "id": 318172112,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672183101
    },
    {
        "content": "<p>From: Andrei Popescu &lt;<a href=\"mailto:andrei.h.popescu@gmail.com\">andrei.h.popescu@gmail.com</a>&gt;</p>\n<blockquote>\n<ol>\n<li>Both type-based and set-based libraries will continue to be maintained and developed in parallel, now with the aid of tools like types-to-sets/types-to-PERs.</li>\n<li>Migration to a single type-based library.</li>\n<li>Migration to a single set-based library.</li>\n</ol>\n</blockquote>\n<p>Well, variant 2 could be done in conjunction with exporting set-based<br>\nresults on demand via types-to-sets. This is the main appeal here:<br>\ndevelop type-based and export set-based.</p>\n<blockquote>\n<p>Also, I agree that much has been said already about the use of alternative foundations and there is no reason to return to this topic. My initial remark was prompted by my observation that the duplication problem seems to be significantly easier to avoid in some of these alternative systems. So, I hope that it was not entirely off-topic.</p>\n</blockquote>\n<p>Not at all off-topic, I didn't mean to suggest that. Of course working<br>\nin ZF would not face these, but other problems.</p>\n<p>Best wishes,<br>\nAndrei</p>",
        "id": 318180915,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672189453
    },
    {
        "content": "<p>From: Ken Kubota &lt;<a href=\"mailto:mail@kenkubota.de\">mail@kenkubota.de</a>&gt;<br>\nThis is due to the restrictions of simple type theory lacking the possibility of quantifying over type variables, and not a matter of type theory in general.<br>\nIf have shown, using some example from group theory, that with lambda binding type variables, too, it is possible to carry out abstract proofs and instantiate them later.<br>\nSee the section \"Type abstraction\" here: <a href=\"https://owlofminerva.net/kubota/software-implementation-of-the-mathematical-logic-r0-available-for-download/\">https://owlofminerva.net/kubota/software-implementation-of-the-mathematical-logic-r0-available-for-download/</a><br>\nAlso: <a href=\"https://sympa.inria.fr/sympa/arc/coq-club/2022-06/msg00025.html\">https://sympa.inria.fr/sympa/arc/coq-club/2022-06/msg00025.html</a></p>\n<p>Kind regards,</p>\n<p>Ken Kubota</p>\n<hr>\n<p>Ken Kubota<br>\n<a href=\"https://doi.org/10.4444/100\">https://doi.org/10.4444/100</a></p>",
        "id": 318288141,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672244700
    },
    {
        "content": "<p>From: Ken Kubota &lt;<a href=\"mailto:mail@kenkubota.de\">mail@kenkubota.de</a>&gt;<br>\nThe whole approach of set theory (generally restricting sets by a rather arbitrary list of non-logical axioms in order to avoid inconsistency) is not systematic.<br>\nMathematics should be expressed naturally, where the restrictions are a matter of the syntax (type theory).<br>\nHowever, simple type theory like HOL or Isabelle/HOL is, of course, too weak. Without introducing some kind of abstraction/quantification over type variables this kind of problems will persist.</p>\n<hr>\n<p>Ken Kubota<br>\n<a href=\"https://doi.org/10.4444/100\">https://doi.org/10.4444/100</a></p>",
        "id": 318288301,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672244764
    },
    {
        "content": "<p>From: Mihails Milehins &lt;<a href=\"mailto:mihailsmilehins@gmail.com\">mihailsmilehins@gmail.com</a>&gt;<br>\nDear Ken Kubota,</p>\n<p>Thank you for your remarks.</p>",
        "id": 318299016,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672249167
    },
    {
        "content": "<p>From: Dominique Unruh &lt;<a href=\"mailto:cl-isabelle-users@lists.cam.ac.uk\">cl-isabelle-users@lists.cam.ac.uk</a>&gt;<br>\nMaybe a variant of variant 2 would be what works on the long run: <br>\nLibraries will contain both set- and type-based definitions (with proper <br>\nlinking via transfer-rules or some related mechanism), but only provide <br>\nlemmas in the type-based language. If the definitions are properly <br>\nlinked, set-based lemmas could be created on the fly (maybe using an <br>\nattribute) without much extra effort. (Because it seems to me that a lot <br>\nof the effort that is not fully automatable in the TTS world is to match <br>\nup the already existing, and somewhat differently flavored, definitions <br>\nin the set- and the type-world.)</p>\n<p>A good example of something where not thinking of TTS when definition <br>\nleads to trouble is \"sum\". The existing definition uses THE and before <br>\nlifting it, one needs to come up with a completely different definition <br>\n(and Mihails and my writeup use different ones, too). Generally, THE and <br>\nSOME should be avoided (and maybe we should have something like <br>\nthe_default that returns a specified value when the predicate does not <br>\nhave unique value). Maybe \"undefined\" is also bad, I am not sure.</p>\n<p>Best wishes,<br>\nDominique.</p>",
        "id": 318311320,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672254614
    },
    {
        "content": "<p>From: Ken Kubota &lt;<a href=\"mailto:mail@kenkubota.de\">mail@kenkubota.de</a>&gt;<br>\nDear Mihails Milehins,</p>\n<p>It is correct that none of the major HOL systems, unfortunately, have moved to type quantification yet, despite the fact that Mike Gordon suggested this in 2001 already.<br>\nThis includes HOL4: <a href=\"https://sourceforge.net/p/hol/mailman/message/37215715/\">https://sourceforge.net/p/hol/mailman/message/37215715/</a> &lt;<a href=\"https://sourceforge.net/p/hol/mailman/message/37215715/\">https://sourceforge.net/p/hol/mailman/message/37215715/</a>&gt;</p>\n<p>HOL-Omega should have enough expressiveness (although I consider the stratification of types as problematic):<br>\n\"using the HOL-Omega system to model monads and prove theorems about their general properties, as well as concepts from category theory such as functors and natural transformations.\"<br>\n<a href=\"http://www.trustworthytools.com/id17.html\">http://www.trustworthytools.com/id17.html</a> &lt;<a href=\"http://www.trustworthytools.com/id17.html\">http://www.trustworthytools.com/id17.html</a>&gt;</p>\n<p>However, unless HOL4 and Isabelle/HOL move to quantification over types, users will either have to live with duplication, or have to resort to other software or to set theory, or spend an increasing amount of time on workarounds such as conversion between type theory and set theory.</p>\n<p>It should be noted that an implementation of quantification over types should use lambda as type quantifier (as suggested by Mike Gordon, and independently of that implemented in my logic R0), and not introduce type quantifiers as new primitive symbols (as described in Tom Melham's paper, which heavily draws on Peter B. Andrews' PhD thesis).</p>\n<p>Kind regards,</p>\n<p>Ken Kubota</p>\n<hr>\n<p>Ken Kubota<br>\n<a href=\"https://doi.org/10.4444/100\">https://doi.org/10.4444/100</a></p>",
        "id": 318342204,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672272771
    },
    {
        "content": "<p>From: Mihails Milehins &lt;<a href=\"mailto:mihailsmilehins@gmail.com\">mihailsmilehins@gmail.com</a>&gt;<br>\nDear All,</p>\n<p>Thank you for further remarks.</p>",
        "id": 318493308,
        "sender_full_name": "Email Gateway",
        "timestamp": 1672352058
    }
]