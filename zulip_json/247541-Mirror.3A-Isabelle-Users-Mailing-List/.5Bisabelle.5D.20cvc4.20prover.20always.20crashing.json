[
    {
        "content": "<p>From: \"Dr A. Koutsoukou-Argyraki\" &lt;<a href=\"mailto:ak2110@cam.ac.uk\">ak2110@cam.ac.uk</a>&gt;<br>\nHi, I've noticed that lately every single time<br>\nI use Sledgehammer, cvc4 always returns the message \"the prover <br>\ncrashed\".<br>\ne, z3, vampire either give me proofs or time out.</p>\n<p>cvc4 simply never works and if my memory serves me well this<br>\nissue must have started with Isabelle 2021.</p>\n<p>Any insights?</p>\n<p>Many thanks<br>\nBest wishes,<br>\nAngeliki</p>",
        "id": 251796124,
        "sender_full_name": "Email Gateway",
        "timestamp": 1630621761
    },
    {
        "content": "<p>From: \"Eugene W. Stark\" &lt;<a href=\"mailto:isabelle-users@starkeffect.com\">isabelle-users@starkeffect.com</a>&gt;<br>\nFrom my point of view, it is definitely the case that the Sledgehammer provers crash more frequently<br>\nand produce failed proofs more frequently in Isabelle2021 than they did in Isabelle2020.  However,<br>\ntaking the whole set of tools in Sledgehammer as an ensemble, much more amazing results are produced<br>\nin Isabelle2021 than in previous versions.  The SMT provers can routinely produce \"one-liners\" that<br>\nreplace tens of lines (or more) of written-out Isar code.  (Of course, you have to throw away all the<br>\nfailed proofs and take the ones that work.)  This capability makes Sledgehammer extremely useful in<br>\nan approach to proof development where the human prover can make proposals for \"big steps\" in a proof,<br>\nwith the expectation that if a proposed assertion is in fact true, then an SMT prover driven from'<br>\nSledgehammer has a very good chance of being able to verify this.</p>\n<p>My personal experience is that SMT-based proofs tend to be rather more fragile with respect to changes<br>\nin the underlying definitions that proofs based on the less-powerful methods, so once a proof has<br>\nbeen sketched out and verified with the help of Sledgehammer, it is well worth the extra effort to go<br>\nthrough and eliminate the uses of SMT and in the process identify reasoning patterns that can be<br>\ncodified in simplification rules, etc.  Nevertheless it is quite a boon to be able to verify a proof plan<br>\nin \"big chunks\" and then come back and refine it, knowing already that it has been verified that<br>\nthe proof is correct in outline.</p>\n<p>For me, the most annoying new behavior in Sledgehammer in Isabelle2021 is the tendency in certain<br>\nsituations to claim to have found a proof and then to spit out Isar code that doesn't even \"show\"<br>\nthe lemma that was supposed to have been proved.  This is apparently some kind of issue in replaying<br>\nin the proof kernel the results of the SMT searches, but as I have very little understanding of<br>\nhow that all works, I will stop there.  Would it be so difficult to ensure that any proposed Isar<br>\nproofs really do \"show\" the lemma that they are supposed to be showing?  I tried early on after<br>\nthe release of Isabelle2021 to capture examples of this, but as it didn't seem like it was helping,<br>\nI gave up on that for the moment.</p>\n<p>I also frequently see failures of the SMT solvers with segmentation faults.  These no doubt<br>\nrepresent programming errors in the SMT solvers.  The wisdom of an LCF-style system with a<br>\n\"trusted kernel\" proves itself over and over again by making it possible to accomplish something<br>\nuseful with very powerful, but potentially buggy, tools.</p>\n<p>I find it generally very difficult \"in the heat of the moment\" when working on a theory<br>\nof several thousand lines that depend on a context of tens of thousands lines of other theories,<br>\nto capture useful repeatable instances of the various ways in which Sledgehammer fails.<br>\nThe randomness in the Sledgehammer procedures, which I think probably derives not only from<br>\nrandomness in the SMT solvers themselves, but also from fact caching mechanisms that at<br>\nwork in Isabelle, makes repeatability problematic in many cases.  I would be interested to hear<br>\nfrom the developers any advice as to how I might be able to provide them with useful failure<br>\ncases to help reduce the number of bugs overall.</p>\n<p>Finally, the timing figures produced by Sledgehammer currently have very little connection to<br>\nreality, in my opinion.  I generally select a particular proposed proof based on the length and<br>\ncontent of the list of facts used, rather than paying too much attention to the timing figures.<br>\nThe discrepancy between the timings reported by Sledgehammer when the proof is proposed and<br>\nthe time available from Isabelle/JEdit seems to have become larger with Isabelle2021, however<br>\nat around the time of the release of Isabelle2021 I started to use a 10-core processor for<br>\ndevelopment, so it is possible that the increased discrepancies have something to do with tallying<br>\nthe time used by the larger number of threads.</p>\n<p>- Gene Stark</p>",
        "id": 251805055,
        "sender_full_name": "Email Gateway",
        "timestamp": 1630628101
    }
]