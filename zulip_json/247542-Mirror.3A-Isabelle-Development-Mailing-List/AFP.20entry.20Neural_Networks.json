[
    {
        "content": "<p>From: Lawrence Paulson via isabelle-dev &lt;<a href=\"mailto:isabelle-dev@mailman.proof.cit.tum.de\">isabelle-dev@mailman.proof.cit.tum.de</a>&gt;<br>\nNeural_Networks is badly broken. I took a look and the failure is in a proof that tests whether a certain generated construction is equal to an explicit construction. They are supposed to be literally identical and are not. I think we should get back to the authors to find out why it failed and how it can be made robust.</p>\n<p>Larry</p>",
        "id": 558250382,
        "sender_full_name": "Email Gateway",
        "timestamp": 1763569608
    },
    {
        "content": "<p>From: \"Achim D. Brucker\" &lt;<a href=\"mailto:adbrucker@0x5f.org\">adbrucker@0x5f.org</a>&gt;<br>\nHi,<br>\nI did a first investigation. Overall, this part of the theory is <br>\nintended to serve as regression test. Thus, it worked as it should: <br>\nacting as early warning for subtle changes that otherwise would go <br>\nundetected, potentially creating \"bit rot\".</p>\n<p>The change is actually due to a difference in Poly/ML used in Isabelle <br>\n2025 vs the current development version (and, 2025-1, for that matter). <br>\nNamely:</p>\n<p>ML\\&lt;open&gt;<br>\n     PackReal32Little.fromBytes (Word8Vector.fromList [0wxC5, 0wx10, <br>\n0wxE8, 0wx3B])<br>\n\\&lt;close&gt;</p>\n<p>returns a different value, namely:</p>\n<ul>\n<li>Isabelle 2025:           val it = 0.007082077209: PackReal32Little.real</li>\n<li>Isabelle development/2025-1:      val it = 0.007082077: <br>\nPackReal32Little.real</li>\n</ul>\n<p>As the rule-of-thumb regarding the precision of 32 IEEE-745 floating <br>\npoint numbers is around 7-8 decimal digits, the new behaviour is OK. <br>\nHence, the fix to get the AFP entry working again is to update the <br>\ntheories to reflect the new values.</p>\n<p>We will take care of this tomorrow.</p>\n<p>Best,</p>\n<p>Achim (and Amy)</p>\n<p>On 19/11/2025 16:25, Lawrence Paulson via isabelle-dev wrote:</p>\n<blockquote>\n<p>Neural_Networks is badly broken. I took a look and the failure is in a <br>\nproof that tests whether a certain generated construction is equal to <br>\nan explicit construction. They are supposed to be literally identical <br>\nand are not. I think we should get back to the authors to find out why <br>\nit failed and how it can be made robust.</p>\n<p>Larry</p>\n</blockquote>",
        "id": 558317360,
        "sender_full_name": "Email Gateway",
        "timestamp": 1763591882
    },
    {
        "content": "<p><strong>From:</strong> Lawrence Paulson via isabelle-dev &lt;<a href=\"mailto:isabelle-dev@mailman.proof.cit.tum.de\">isabelle-dev@mailman.proof.cit.tum.de</a>&gt;</p>\n<p>I find it surprising that the meaning of a specific bit pattern as a real number according to an established floating-point standard can change from year to year. How do we know it won't change again?</p>\n<p>Larry</p>\n<blockquote>\n<p>On 19 Nov 2025, at 22:37, Achim D. Brucker &lt;<a href=\"mailto:adbrucker@0x5f.org\">adbrucker@0x5f.org</a>&gt; wrote:</p>\n<p>Hi,<br>\nI did a first investigation. Overall, this part of the theory is intended to serve as regression test. Thus, it worked as it should: acting as early warning for subtle changes that otherwise would go undetected, potentially creating \"bit rot\". </p>\n<p>The change is actually due to a difference in Poly/ML used in Isabelle 2025 vs the current development version (and, 2025-1, for that matter). Namely:</p>\n<p>ML\\&lt;open&gt;<br>\n    PackReal32Little.fromBytes (Word8Vector.fromList [0wxC5, 0wx10, 0wxE8, 0wx3B])<br>\n\\&lt;close&gt;</p>\n<p>returns a different value, namely:</p>\n<ul>\n<li>Isabelle 2025:                                  val it = 0.007082077209: PackReal32Little.real</li>\n<li>Isabelle development/2025-1:      val it = 0.007082077: PackReal32Little.real</li>\n</ul>\n<p>As the rule-of-thumb regarding the precision of 32 IEEE-745 floating point numbers is around 7-8 decimal digits, the new behaviour is OK. Hence, the fix to get the AFP entry working again is to update the theories to reflect the new values. <br>\nWe will take care of this tomorrow. </p>\n<p>Best,<br>\nAchim (and Amy) </p>\n<p>On 19/11/2025 16:25, Lawrence Paulson via isabelle-dev wrote:</p>\n<blockquote>\n<p>Neural_Networks is badly broken. I took a look and the failure is in a proof that tests whether a certain generated construction is equal to an explicit construction. They are supposed to be literally identical and are not. I think we should get back to the authors to find out why it failed and how it can be made robust.</p>\n<p>Larry<br>\n</p>\n</blockquote>\n</blockquote>",
        "id": 558407522,
        "sender_full_name": "Email Gateway",
        "timestamp": 1763638610
    }
]