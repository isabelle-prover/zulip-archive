[
    {
        "content": "<p>From: \"Eugene W. Stark\" &lt;<a href=\"mailto:stark@cs.stonybrook.edu\">stark@cs.stonybrook.edu</a>&gt;<br>\nSince there is a thread going about various kinds of hanging, I would<br>\nlike to ask about behavior that I have experienced (with Isabelle2014).<br>\nBasically the symptom is that the prover becomes unresponsive, though JEdit<br>\ndoes still respond.  Often, the entire buffer becomes highlighted in pink,<br>\nand many times it is impossible to continue without exiting JEdit<br>\n(thereby exiting the prover) and restarting.  While in the stuck state,<br>\nthe prover (Poly process) exhibits constant CPU consumption.  It does not<br>\nappear to be a thrashing issue because the disk traffic is low and CPU<br>\nutilization high.</p>\n<p>I have only rarely seen this problem on my home system, which is a year-old<br>\ni5 processor with four cores (I am running Ubuntu 14.04 LTS) and 8GB RAM.<br>\nHowever, it occurs much more often on my office machine, which is a venerable<br>\n32-bit, one-core system with 2GB (also running Ubuntu 14.04 LTS).<br>\nIn fact, it is pretty much impossible to use \"try\" on that machine because<br>\nit almost always results in a stuck prover.  Once the situation has arisen,<br>\nattempting to delete the \"try\" from the JEdit buffer does not successfully<br>\ncancel the ongoing proof attempt.</p>\n<p>I would be interested to know what I should be looking for in order to try<br>\nto solve this.  When I noticed it this morning on my home system, it appeared<br>\nto be because there were several threads occupied by various proof attempts<br>\n(I think \"blast\") that did not terminate.  In this particular case I was<br>\nable to recover by erasing the \"by blast\" lines, but when things are truly<br>\nstuck (especially on the one-core system) this does not help.</p>\n<p>Another situation I have gotten into that produced similar symptoms is when<br>\nI inadvertently created recursive simplifier rules.  However, I don't think<br>\nthis is usually the issue.</p>\n<p>My question is: is it supposed to be possible for prover threads to hang<br>\naround indefinitely (purple mark in right margin and proof method highlighted<br>\nin purple) without timing out in some reasonable period?  If so, why,<br>\nand what should I look for to try to fix the problem?  If not, then this would<br>\nseem to be an issue that should be looked into.</p>\n<p>Thanks for the help.</p>\n<p>- Gene Stark</p>",
        "id": 294634266,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661159506
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOn Tue, 14 Apr 2015, Eugene W. Stark wrote:</p>\n<blockquote>\n<p>Since there is a thread going about various kinds of hanging, I would <br>\nlike to ask about behavior that I have experienced (with Isabelle2014). <br>\nBasically the symptom is that the prover becomes unresponsive, though <br>\nJEdit does still respond.  Often, the entire buffer becomes highlighted <br>\nin pink, and many times it is impossible to continue without exiting <br>\nJEdit (thereby exiting the prover) and restarting.  While in the stuck <br>\nstate, the prover (Poly process) exhibits constant CPU consumption.  It <br>\ndoes not appear to be a thrashing issue because the disk traffic is low <br>\nand CPU utilization high.</p>\n</blockquote>\n<p>This description generally sounds like a forced break due to the Poly/ML <br>\nprocess doing garbage collection in a tight memory situation.  The <br>\nautomatic heap resizing usually stays below the available physical memory, <br>\nto avoid disk thrashing for VM use, but this comes at the cost of quite <br>\naggressive attempts to reclaim heap space. It can take 0.5 to 1 min to do <br>\nso, and the grey-pink to disappear.</p>\n<blockquote>\n<p>However, it occurs much more often on my office machine, which is a <br>\nvenerable 32-bit, one-core system with 2GB (also running Ubuntu 14.04 <br>\nLTS).</p>\n</blockquote>\n<p>That machine is indeed very old and small.  I've just made some <br>\nexperiments with Isabelle2015-RC1 on some virtual machine of that size. <br>\nIt basically works for things like HOL/Library or HOL/Decision_Procs, but <br>\nis not much fun.</p>\n<p>It becomes impossible if the Linux system is actually running a 64bit OS. <br>\nIn that case you should make sure that the 32bit C/C++ libraries are <br>\ninstalled, to avoid doubling memory usage of the Poly/ML process.</p>\n<p>These days the bottom line is at approx. 2 cores, 4 GB -- i.e. the oldest <br>\nmachine that I happen to have around for testing (from 2009).</p>\n<blockquote>\n<p>I would be interested to know what I should be looking for in order to try<br>\nto solve this.  When I noticed it this morning on my home system, it appeared<br>\nto be because there were several threads occupied by various proof attempts<br>\n(I think \"blast\") that did not terminate.  In this particular case I was<br>\nable to recover by erasing the \"by blast\" lines, but when things are truly<br>\nstuck (especially on the one-core system) this does not help.</p>\n<p>Another situation I have gotten into that produced similar symptoms is when<br>\nI inadvertently created recursive simplifier rules.  However, I don't think<br>\nthis is usually the issue.</p>\n<p>My question is: is it supposed to be possible for prover threads to hang <br>\naround indefinitely (purple mark in right margin and proof method <br>\nhighlighted in purple) without timing out in some reasonable period? <br>\nIf so, why, and what should I look for to try to fix the problem?  If <br>\nnot, then this would seem to be an issue that should be looked into.</p>\n</blockquote>\n<p>Of course, proof tools can hang around indefinitely in their potential <br>\nnon-termination. This is inherent in the game we are playing here.  E.g. <br>\nan ill-behaved \"by simp\" or \"by blast\" will continue sucking up CPU <br>\nresources, until it is removed from the proof document.</p>\n<p>The \"auto\" tools of the IDE are slightly different (auto quickcheck, auto <br>\nmethods etc.): they have a builtin timeout.  Sometimes it can still take <br>\nlonger than expected to terminate non-terminating tools.  I think that <br>\n\"auto methods\" is particularly dangerous here, but it is not enabled by <br>\ndefault.  (It is a variant of 'try'.)</p>\n<p>Note that Isabelle2015-RC1 has various refinements in the scheduling of <br>\ntasks.  So it is worth trying to see if it makes a difference -- or if it <br>\nhas newly introduced problems in the attempt to solve old problems.</p>\n<p>Makarius</p>",
        "id": 294635046,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661159778
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nBTW, Windows 7 works quite well for such old hardware: the IDE is also a <br>\nbit more smooth, since the graphics sub-system is better supported by <br>\nJava/AWT.</p>\n<p>Makarius</p>",
        "id": 294635058,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661159784
    },
    {
        "content": "<p>From: \"Eugene W. Stark\" &lt;<a href=\"mailto:stark@cs.stonybrook.edu\">stark@cs.stonybrook.edu</a>&gt;<br>\nAs a potentially useful point of information regarding the subject problem,<br>\nafter reading some recent messages on the list that talked about the possibility<br>\nof changing the maximum number of prover threads, I went on my office machine<br>\n(the very old one-core Xeon with hyperthreading), changed the number of threads<br>\nto 2 (it was 0) and did a \"isabelle build -s HOL\".  Now when I run Isabelle and do:</p>\n<p>ML {* Multithreading.max_threads_value () *}</p>\n<p>it reports 2 instead of 1.  In addition, it is now possible to cancel ongoing<br>\n\"try\" without the looping/hanging occurring as it was before the change.<br>\nThe machine is slow, obviously, but it is usable.</p>\n<p>So I think there might be an issue (in Isabelle2014) that shows up when the<br>\nmaximum number of prover threads is 1.  Maybe this information will help to<br>\nidentify it or at least help somebody else in the same situation.</p>\n<p>- Gene Stark</p>",
        "id": 294636920,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661160526
    }
]