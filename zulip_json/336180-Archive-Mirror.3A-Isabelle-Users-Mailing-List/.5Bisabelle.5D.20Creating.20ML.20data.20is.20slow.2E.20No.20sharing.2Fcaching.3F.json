[
    {
        "content": "<p>From: \"C. Diekmann\" &lt;<a href=\"mailto:diekmann@in.tum.de\">diekmann@in.tum.de</a>&gt;<br>\nDear ML and Isabelle experts,</p>\n<p>I observed that my code written in Isabelle got horribly slow after a<br>\nsmall change. Currently, I evaluate my code with</p>\n<p>value[code]</p>\n<p>Here is a small Isabelle/HOL example that illustrates the issue:</p>\n<p>datatype 'a expr = Prim 'a | And \"'a expr\" \"'a expr\"</p>\n<p>definition add_expr :: \"'a expr ⇒ 'a expr list ⇒ 'a expr list\"<br>\n  where \"add_expr e es = map (λe'. And e e') es\"</p>\n<p>1) Constructing a list of expressions is quite fast:<br>\nvalue[code] \"map Prim [1..1000]\" (*0.339s elapsed time, 0.678s cpu<br>\ntime, 0.000s GC time*)</p>\n<p>2) If I add a small expression to a large list of expressions, it is also fast:<br>\nvalue[code] \"add_expr (Prim 42) (map Prim [1..1000])\" (*0.604s elapsed<br>\ntime, 1.201s cpu time, 0.026s GC time*)</p>\n<p>3) Constructing one larger expression is also fast:<br>\nvalue[code] \"fold And (map Prim [1..10]) (Prim 0)\" (*too fast to give<br>\na timing?*)</p>\n<p>4) Adding bigger expressions to a list of expressions is more than 10<br>\ntimes slower than (2):<br>\nvalue[code] \"add_expr (fold And (map Prim [1..10]) (Prim 0)) (map Prim<br>\n[1..1000])\" (<em>5.853s elapsed time, 11.495s cpu time, 0.316s GC time</em>)</p>\n<p>5) Almost the same result if the expression is written down explicitly:<br>\nvalue[code] \"add_expr (And (Prim 10) (And (Prim 9) (And (Prim 8) (And<br>\n(Prim 7) (And (Prim 6) (And (Prim 5) (And (Prim 4) (And (Prim 3) (And<br>\n(Prim 2) (And (Prim 1) (Prim 0))))))))))) (map Prim [1..1000])\"<br>\n(<em>5.715s elapsed time, 11.148s cpu time, 0.316s GC time</em>)</p>\n<p>I played around with this, repeated these small measurements and<br>\nshuffled the value statements to make sure that we are not observing<br>\nsome CPU turbo boost artifact here.</p>\n<p>By the way, the performance doesn't seem to get worse If the list of<br>\nexpressions consists of large expressions:<br>\nvalue[code] \"add_expr (fold And (map Prim [1..10]) (Prim 0)) (map (λi.<br>\n(fold And (map Prim [1..10]) (Prim i))) [1..1000])\" (*5.291s elapsed<br>\ntime, 6.138s cpu time, 0.366s GC time*)</p>\n<p>Why is (4) so slow, compared to (2)?</p>\n<p>How can I speed this up? Currently, this renders my theory unusable.</p>\n<p>Doesn't ML use sharing or caching? I always assumed that, if I have<br>\nexpressions e1 and e2, then constructing \"And e1 e2\" would reuse the<br>\nvalues e1 and e2 and this operation would be independent of the sizes<br>\nof e1 and e2.</p>\n<p>Best,<br>\n  Cornelius</p>",
        "id": 294648120,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661164960
    },
    {
        "content": "<p>From: Christian Sternagel &lt;<a href=\"mailto:c.sternagel@gmail.com\">c.sternagel@gmail.com</a>&gt;<br>\nDear Cornelius,</p>\n<p>isn't the problem merely that the expression</p>\n<p>\"fold And (map Prim [1..10]) (Prim 0)\"</p>\n<p>is evaluated 1000 times? I guess you could speed-up things by using <br>\nsomething like:</p>\n<p>\"let e = fold And (map Prim [1..10]) (Prim 0) in add_expr e (map Prim <br>\n[1..1000]\"</p>\n<p>I did not yet test my intuition though.</p>\n<p>cheers</p>\n<p>chris</p>",
        "id": 294648134,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661164967
    },
    {
        "content": "<p>From: Christian Sternagel &lt;<a href=\"mailto:c.sternagel@gmail.com\">c.sternagel@gmail.com</a>&gt;<br>\nSorry for the noise, I'm wrong. -cheers chris</p>",
        "id": 294648141,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661164971
    },
    {
        "content": "<p>From: Andreas Lochbihler &lt;<a href=\"mailto:andreas.lochbihler@inf.ethz.ch\">andreas.lochbihler@inf.ethz.ch</a>&gt;<br>\nHi Cornelius,</p>\n<p>Your problem is not the evaluation, it's the pretty printing. Evaluations 4 and 5 result <br>\nin quite large terms, which have a fairly deep nesting of functions. The Isabelle/ML <br>\nprocess sends to Isabelle/jEdit the whole syntax tree, which includes type annotations for <br>\nevery subterm and layout information (suitable positions for line breaks). All this <br>\ninformation has to be parsed and type-set in the output buffer.</p>\n<p>If you really want to measure the evaluation time, you should either make sure that your <br>\nevaluations result in small terms or use the ML interface which avoids most of the <br>\npretty-printing overhead.  For example,</p>\n<p>value [code]<br>\n     \"let x = add_expr (fold And (map Prim [1..10]) (Prim 0)) (map Prim [1..1000]) in ()\"</p>\n<p>is pretty fast (note that HOL's let gets compiled to ML's let, so the let binding is <br>\nalways evaluated) and so is</p>\n<p>definition \"four = add_expr (fold And (map Prim [1..10]) (Prim 0)) (map Prim [1..1000])\"<br>\n   ML ‹ @{code four} ›</p>\n<p>I do not know the details of the implementation in Isabelle/Scala, so I cannot tell why 2 <br>\nis much faster than 4 in terms of pretty printing. My guess is that 2 consists of a long <br>\nlist of values, i.e., you have a fairly flat term structure and special notation for lists <br>\n(which suppresses a lot of the auxiliary information passed along with the constants).</p>\n<p>Hope this helps,<br>\nAndreas</p>",
        "id": 294648196,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661165003
    },
    {
        "content": "<p>From: \"C. Diekmann\" &lt;<a href=\"mailto:diekmann@in.tum.de\">diekmann@in.tum.de</a>&gt;</p>\n<blockquote>\n<p>Your problem is not the evaluation, it's the pretty printing.</p>\n</blockquote>\n<p>Thanks Andreas! This explains everything.</p>\n<p>Cheers,<br>\n  Cornelius</p>",
        "id": 294648257,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661165033
    },
    {
        "content": "<p>From: \"C. Diekmann\" &lt;<a href=\"mailto:diekmann@in.tum.de\">diekmann@in.tum.de</a>&gt;<br>\nDear list,</p>\n<p>pretty printing can cause quite some overhead. I just wanted to</p>\n<p>(<em>2174.839s elapsed time, 3308.410s cpu time, 161.539s GC time</em>)<br>\nvalue[code] \"unfolded\"</p>\n<p>(<em>0.561s</em>)<br>\nvalue[code] \"let x = unfolded in ()\"</p>\n<p>(<em>39.257s</em>)<br>\nvalue[code] \"map (my_toString) unfolded\"</p>",
        "id": 294648348,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661165058
    },
    {
        "content": "<p>From: \"C. Diekmann\" &lt;<a href=\"mailto:diekmann@in.tum.de\">diekmann@in.tum.de</a>&gt;<br>\n... I just wanted to share those numbers in the previous mail. I never<br>\nthought that writing a custom toString function can make such a huge<br>\ndifference. 30min vs. 30sec. The tested data is a ~500 rules iptables<br>\nfirewall [1]</p>\n<p>Cornelius</p>\n<p>(and probably I should unify my mail clients key binding to avoid<br>\nthose half-finished mails. Sorry for the noise.)</p>\n<p>[1] <a href=\"https://github.com/diekmann/Iptables_Semantics\">https://github.com/diekmann/Iptables_Semantics</a> paper at FM'15</p>",
        "id": 294648361,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661165065
    },
    {
        "content": "<p>From: Gerwin Klein &lt;<a href=\"mailto:Gerwin.Klein@nicta.com.au\">Gerwin.Klein@nicta.com.au</a>&gt;<br>\nWe’ve observed similar things in the past with printing large goal states and Simpl terms, esp when lots of abbreviations are active.</p>\n<p>It has gotten below the pain threshold for us, I think mainly after Makarius reorganised the whole syntax stack a few years ago, so we haven’t done anything about it in a long time. (Maybe we just got used to it too much).</p>\n<p>If someone feels motivated to look at what exactly is taking so long in printing and if there is anything that could be done about it, we’d be happy to help and dig out examples.</p>\n<p>Cheers,<br>\nGerwin</p>\n<hr>\n<p>The information in this e-mail may be confidential and subject to legal professional privilege and/or copyright. National ICT Australia Limited accepts no liability for any damage caused by this email or its attachments.</p>",
        "id": 294648370,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661165071
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nThat is occasionally called \"hash consing\", but it would introduce a <br>\ncentral (synchronized) place for construction data structures.  Not very <br>\nefficient in the multicore era ...</p>\n<p>Even in ancient single-core times, this old idea did not became very <br>\npopular, probably because most data is rather short-lived and it is better <br>\nnot to make such a fuss about its allocation.</p>\n<p>The Poly/ML runtime-system has other means to organize long-time survivers <br>\non the heap efficiently, by re-introducing sharing on demand.  You won't <br>\nsee that in such micro-benchmarks, though.  In AFP/JinjaThreads you do see <br>\nthe CPU crunching a lot, to fit the resulting heap into the tiny 4 GB <br>\naddress space: the Monitor/Heap dockable in Isabelle/jEdit gives an <br>\nimpression of what is going on in the ML RTS.</p>\n<p>Makarius</p>",
        "id": 294654283,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661167370
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nI've done some ML timing here <br>\n<a href=\"http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l58\">http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l58</a><br>\nand here <br>\n<a href=\"http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l61\">http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l61</a><br>\nand here<br>\n<a href=\"http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l64\">http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l64</a></p>\n<p>The timing for value_maybe_select is about the same as the pretty_term + <br>\nwriteln together.  Looking at the profile of value_maybe_select, there are <br>\na lot of Simplifier operations, which probably belong to the preparation <br>\nstage of the code-generator, including preparation of the result before <br>\nactual printing.</p>\n<p>So my guess is that \"let x = add_expr ...\" is so much faster, because its <br>\nresult is so small, independtly of printing.</p>\n<p>In the above timing experiments, Isabelle/Scala is not measured at all. <br>\nThe front-end is asynchronous.  It might burn a lot of CPU cycles on its <br>\nown for displaying big output, but that is not directly seen here. I don't <br>\nthink this particularly example is big enough to be a big problem.  Other <br>\nexamples can bomb the front-end as usual.</p>\n<p>Makarius</p>",
        "id": 294654295,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661167377
    },
    {
        "content": "<p>From: Andreas Lochbihler &lt;<a href=\"mailto:andreas.lochbihler@inf.ethz.ch\">andreas.lochbihler@inf.ethz.ch</a>&gt;<br>\nHi Makarius,</p>\n<p>The code generator does quite some post-processing of the term output using the simplifier <br>\n(by unfolding the equations in code_post). You might be right in that this kind of <br>\npost-processing is the main bottleneck here. Technically, this is not part of the pretty <br>\nprinter, but I would still consider this as a part of pretty-printing after evaluation.</p>\n<p>Preprocessing of the code equations can take considerable time in some cases, but AFAIK <br>\nFlorian implemented a cache for code equations. Thus, a second invocation of \"value <br>\n[code]\" for the same term should not suffer from preprocessing time.</p>\n<p>Best,<br>\nAndreas</p>",
        "id": 294654306,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661167382
    },
    {
        "content": "<p>From: Florian Haftmann &lt;<a href=\"mailto:florian.haftmann@informatik.tu-muenchen.de\">florian.haftmann@informatik.tu-muenchen.de</a>&gt;</p>\n<blockquote>\n<p>Preprocessing of the code equations can take considerable time in some<br>\ncases, but AFAIK Florian implemented a cache for code equations. Thus, a<br>\nsecond invocation of \"value [code]\" for the same term should not suffer<br>\nfrom preprocessing time.</p>\n</blockquote>\n<p>Note that the conversions involving code generation are built like<br>\nsandwiches on top of each other, each adding a further level of<br>\npreparation and postpolishing to the other.  For illustration, Try to<br>\ndowntrack the dependencies of e.g. Code_Runtime.dynamic_value.  You<br>\nmight consider plugging in some time measurements there to get an idea<br>\nwhat is really going on.</p>\n<p>Florian</p>\n<blockquote>\n<p>Best,<br>\nAndreas</p>\n<p>On 03/09/15 16:11, Makarius wrote:</p>\n<blockquote>\n<p>On Wed, 29 Jul 2015, Andreas Lochbihler wrote:</p>\n<blockquote>\n<p>If you really want to measure the evaluation time, you should either<br>\nmake sure that your<br>\nevaluations result in small terms or use the ML interface which<br>\navoids most of the<br>\npretty-printing overhead.  For example,</p>\n<p>value [code]<br>\n    \"let x = add_expr (fold And (map Prim [1..10]) (Prim 0)) (map Prim<br>\n    [1..1000]) in ()\"</p>\n<p>is pretty fast (note that HOL's let gets compiled to ML's let, so the<br>\nlet binding is<br>\nalways evaluated) and so is</p>\n<p>definition \"four = add_expr (fold And (map Prim [1..10]) (Prim 0))<br>\n(map<br>\n  Prim [1..1000])\"<br>\n  ML ‹ @{code four} ›</p>\n<p>I do not know the details of the implementation in Isabelle/Scala, so<br>\nI cannot tell why<br>\n2 is much faster than 4 in terms of pretty printing. My guess is that<br>\n2 consists of a<br>\nlong list of values, i.e., you have a fairly flat term structure and<br>\nspecial notation<br>\nfor lists (which suppresses a lot of the auxiliary information passed<br>\nalong with the<br>\nconstants).</p>\n</blockquote>\n<p>I've done some ML timing here<br>\n<a href=\"http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l58\">http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l58</a></p>\n<p>and here<br>\n<a href=\"http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l61\">http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l61</a></p>\n<p>and here<br>\n<a href=\"http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l64\">http://isabelle.in.tum.de/repos/isabelle/file/Isabelle2015/src/HOL/Tools/value.ML#l64</a></p>\n<p>The timing for value_maybe_select is about the same as the pretty_term<br>\n+ writeln<br>\ntogether.  Looking at the profile of value_maybe_select, there are a<br>\nlot of Simplifier<br>\noperations, which probably belong to the preparation stage of the<br>\ncode-generator,<br>\nincluding preparation of the result before actual printing.</p>\n<p>So my guess is that \"let x = add_expr ...\" is so much faster, because<br>\nits result is so<br>\nsmall, independtly of printing.</p>\n<p>In the above timing experiments, Isabelle/Scala is not measured at<br>\nall. The front-end is<br>\nasynchronous.  It might burn a lot of CPU cycles on its own for<br>\ndisplaying big output, but<br>\nthat is not directly seen here. I don't think this particularly<br>\nexample is big enough to<br>\nbe a big problem.  Other examples can bomb the front-end as usual.</p>\n<p>Makarius</p>\n</blockquote>\n<p><a href=\"/user_uploads/14278/0nEwqnGSW4i1zvOUpNAtjMMv/signature.asc\">signature.asc</a></p>\n</blockquote>",
        "id": 294654606,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661167524
    }
]