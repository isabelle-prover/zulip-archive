[
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOn 12/04/2020 20:53, Peter Lammich wrote:</p>\n<blockquote>\n<p>I was interrupted by a \"Prover process<br>\nterminated with error\" dialogue displaying the message below.</p>\n<p>Welcome to Isabelle/HOL (Isabelle2020-RC5: April 2020)<br>\nInterrupt<br>\nInterrupt<br>\nInterrupt<br>\nstandard_error terminated<br>\nstandard_output terminated<br>\nprocess terminated<br>\nMalformed message:<br>\nbad chunk (unexpected EOF after 679 of 836 bytes)<br>\nmessage_output terminated<br>\ncommand_input terminated<br>\nprocess_manager terminated<br>\nReturn code: 137</p>\n</blockquote>\n<p>The return code 137 = 128 + 9 means that the poly process was terminated by a<br>\nhard kill, i.e. the runtime system did not react to anymore to SIGINT or SIGTERM.</p>\n<p>Moreover, the Interrupt exceptions on the syslog look suspicious: it indicates<br>\nresource problems in ML, e.g. severe shortage of heap or stack.</p>\n<p>What is the output the following?</p>\n<p>* isabelle getenv ML_PLATFORM ML_OPTIONS</p>\n<p>* ulimit -a</p>\n<blockquote>\n<p>On the command line, I see</p>\n<p>j65266pl@cspool450:~/devel/isabelle/RF2/llvm$ ~/opt/Isabelle2020-<br>\nRC5/bin/isabelle jedit -d ~/devel/isabelle/afp-devel/thys/<br>\nbasic/LLVM_Basic_Main.thy</p>\n</blockquote>\n<p>Where can I get Isabelle/LLVM for Isabelle2020?</p>\n<p>Makarius</p>",
        "id": 294826910,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661244872
    },
    {
        "content": "<p>From: Peter Lammich &lt;<a href=\"mailto:lammich@in.tum.de\">lammich@in.tum.de</a>&gt;</p>\n<blockquote>\n<p>What is the output the following?</p>\n<p>* isabelle getenv ML_PLATFORM ML_OPTIONS</p>\n<p>* ulimit -a</p>\n</blockquote>\n<p>j65266pl@cspool450:~$ ~/opt/Isabelle2020-RC5/bin/isabelle getenv<br>\nML_PLATFORM ML_OPTIONS<br>\nML_PLATFORM=x86_64_32-linux<br>\nML_OPTIONS=--minheap 500<br>\nj65266pl@cspool450:~$ ulimit -a<br>\ncore file size          (blocks, -c) 0<br>\ndata seg size           (kbytes, -d) unlimited<br>\nscheduling priority             (-e) 0<br>\nfile size               (blocks, -f) unlimited<br>\npending signals                 (-i) 127434<br>\nmax locked memory       (kbytes, -l) 16384<br>\nmax memory size         (kbytes, -m) unlimited<br>\nopen files                      (-n) 1024<br>\npipe size            (512 bytes, -p) 8<br>\nPOSIX message queues     (bytes, -q) 819200<br>\nreal-time priority              (-r) 0<br>\nstack size              (kbytes, -s) 8192<br>\ncpu time               (seconds, -t) unlimited<br>\nmax user processes              (-u) 127434<br>\nvirtual memory          (kbytes, -v) unlimited<br>\nfile locks                      (-x) unlimited</p>\n<blockquote>\n<p>Where can I get Isabelle/LLVM for Isabelle2020?</p>\n</blockquote>\n<p><a href=\"https://github.com/lammich/isabelle_llvm\">https://github.com/lammich/isabelle_llvm</a></p>\n<p>However, the session that failed used a private copy with some<br>\nexperimental changes. Anyway, the error seems not reproducible.</p>",
        "id": 294826923,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661244878
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOn 13/04/2020 17:44, Peter Lammich wrote:</p>\n<blockquote>\n<blockquote>\n<p>What is the output the following?</p>\n<p>* isabelle getenv ML_PLATFORM ML_OPTIONS</p>\n<p>* ulimit -a</p>\n</blockquote>\n<p>j65266pl@cspool450:~$ ~/opt/Isabelle2020-RC5/bin/isabelle getenv<br>\nML_PLATFORM ML_OPTIONS<br>\nML_PLATFORM=x86_64_32-linux<br>\nML_OPTIONS=--minheap 500<br>\nj65266pl@cspool450:~$ ulimit -a<br>\ncore file size          (blocks, -c) 0<br>\ndata seg size           (kbytes, -d) unlimited<br>\nscheduling priority             (-e) 0<br>\nfile size               (blocks, -f) unlimited<br>\npending signals                 (-i) 127434<br>\nmax locked memory       (kbytes, -l) 16384<br>\nmax memory size         (kbytes, -m) unlimited<br>\nopen files                      (-n) 1024<br>\npipe size            (512 bytes, -p) 8<br>\nPOSIX message queues     (bytes, -q) 819200<br>\nreal-time priority              (-r) 0<br>\nstack size              (kbytes, -s) 8192<br>\ncpu time               (seconds, -t) unlimited<br>\nmax user processes              (-u) 127434<br>\nvirtual memory          (kbytes, -v) unlimited<br>\nfile locks                      (-x) unlimited</p>\n</blockquote>\n<p>The parameters are fine -- the usual defaults on Linux.</p>\n<blockquote>\n<blockquote>\n<p>Where can I get Isabelle/LLVM for Isabelle2020?</p>\n</blockquote>\n<p><a href=\"https://github.com/lammich/isabelle_llvm\">https://github.com/lammich/isabelle_llvm</a></p>\n<p>However, the session that failed used a private copy with some<br>\nexperimental changes. Anyway, the error seems not reproducible.</p>\n</blockquote>\n<p>The point is the get an impression about overall resource usage. After playing<br>\nwith it for 30min, it looks all fine and smooth to me.</p>\n<p>There is significant usage of ML heap, though. On a mobile machine with 16GB<br>\ntotal, the heap converges towards 10-11GB.</p>\n<p>I have occasionally seen instabilities of the Poly/ML runtime system in batch<br>\nbuilds in such rather full situations.</p>\n<p>For now, I would say we keep watching it carefully. David Matthews has done a<br>\nlot of fine-tuning of heap management recently, and a bit more is to be<br>\nexpected soon (after the Isabelle2020 release).</p>\n<p>Makarius</p>",
        "id": 294826948,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661244896
    },
    {
        "content": "<p>From: Peter Lammich &lt;<a href=\"mailto:lammich@in.tum.de\">lammich@in.tum.de</a>&gt;</p>\n<blockquote>\n<p>There is significant usage of ML heap, though. On a mobile machine<br>\nwith 16GB<br>\ntotal, the heap converges towards 10-11GB.</p>\n<p>I have occasionally seen instabilities of the Poly/ML runtime system<br>\nin batch<br>\nbuilds in such rather full situations.</p>\n</blockquote>\n<p>I have 32GB of memory.</p>\n<blockquote>\n<p>For now, I would say we keep watching it carefully. David Matthews<br>\nhas done a<br>\nlot of fine-tuning of heap management recently, and a bit more is to<br>\nbe<br>\nexpected soon (after the Isabelle2020 release).</p>\n</blockquote>\n<p>Ok, I'll watch it. Right now, it looks that it was a one-in-a-million<br>\nthing, as it did not (yet) occur again.</p>",
        "id": 294826959,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661244902
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOK, I also have 32GB on my main machine and will continue testing with that.</p>\n<p>Note that Poly/ML in x86_64_32 mode (default) is limited to approx. 16GB, and<br>\nthese 10-11GB user heap are rather typical in practice. David Matthews has<br>\nsome ideas in the pipeline to double that space --- it will be flushed the<br>\nnext time there is sufficient funding for such a project.</p>\n<p>Which base logic image are you typically using for editing Isabelle/LLVM?</p>\n<p>Makarius</p>",
        "id": 294826968,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661244908
    },
    {
        "content": "<p>From: Peter Lammich &lt;<a href=\"mailto:lammich@in.tum.de\">lammich@in.tum.de</a>&gt;</p>\n<blockquote>\n<p>Which base logic image are you typically using for editing<br>\nIsabelle/LLVM?</p>\n</blockquote>\n<p>Typically, afp/Refine_Monadic.</p>\n<p>When the error happened, I had only loaded vcg/LLVM_VCG_Main and <br>\na few other theories I'm currently working on, from the default HOL<br>\nimage.</p>\n<p>Peter</p>\n<blockquote>\n<p>Makarius<br>\n</p>\n</blockquote>",
        "id": 294826982,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661244920
    },
    {
        "content": "<p>From: Peter Lammich &lt;<a href=\"mailto:lammich@in.tum.de\">lammich@in.tum.de</a>&gt;<br>\nHi, <br>\nI got the error again, and this time I investigated it a bit. Linux<br>\ninvoked OOM-killer on the poly-process! I attached the log, the last 3<br>\nmessages show which process was selected for killing.There were two<br>\nIsabelle's running, and OOM-killer picked one.<br>\nCan I somehow tell poly to do more aggressive garbage collection,<br>\nBEFORE it gets OOM killed?<br>\nWhat changed in the poly configuration? I never had those problems with<br>\nIsabelle-2019, and sometimes had up to 3 instances running in parallel.<br>\nHow can I find out if I changed something? A diff between the<br>\n.isabelle-folders reveals nothing suspicious to me?</p>\n<p>Best,  Peter</p>\n<p>------------------Apr 16 23:58:12 cspool450<br>\norg.gnome.Shell.desktop[3621]: Memory pressure relief: Total: res =<br>\n34840576/34803712/-36864, res+swap = 28217344/28217344/0Apr 16 23:58:17<br>\ncspool450 kernel: PacerThread invoked oom-killer:<br>\ngfp_mask=0x100cca(GFP_HIGHUSER_MOVABLE), order=0, oom_score_adj=300Apr<br>\n16 23:58:17 cspool450 kernel: CPU: 6 PID: 28464 Comm: PacerThread Not<br>\ntainted 5.3.0-46-generic #38~18.04.1-UbuntuApr 16 23:58:17 cspool450<br>\nkernel: Hardware name: Dell Inc. Precision 3540/0M14W7, BIOS 1.6.5<br>\n12/26/2019Apr 16 23:58:17 cspool450 kernel: Call Trace:Apr 16 23:58:17<br>\ncspool450 kernel:  dump_stack+0x6d/0x95Apr 16 23:58:17 cspool450<br>\nkernel:  dump_header+0x4f/0x200Apr 16 23:58:17 cspool450<br>\nkernel:  oom_kill_process+0xe6/0x120Apr 16 23:58:17 cspool450<br>\nkernel:  out_of_memory+0x109/0x510Apr 16 23:58:17 cspool450<br>\nkernel:  __alloc_pages_slowpath+0xad1/0xe10Apr 16 23:58:17 cspool450<br>\nkernel:  ? __switch_to_asm+0x34/0x70Apr 16 23:58:17 cspool450<br>\nkernel:  __alloc_pages_nodemask+0x2cd/0x320Apr 16 23:58:17 cspool450<br>\nkernel:  alloc_pages_current+0x6a/0xe0Apr 16 23:58:17 cspool450<br>\nkernel:  __page_cache_alloc+0x6a/0xa0Apr 16 23:58:17 cspool450<br>\nkernel:  pagecache_get_page+0x9c/0x2b0Apr 16 23:58:17 cspool450<br>\nkernel:  filemap_fault+0x3a2/0xb00Apr 16 23:58:17 cspool450 kernel:  ?<br>\nunlock_page_memcg+0x12/0x20Apr 16 23:58:17 cspool450 kernel:  ?<br>\npage_add_file_rmap+0x5e/0x150Apr 16 23:58:17 cspool450 kernel:  ?<br>\nalloc_set_pte+0x52e/0x5f0Apr 16 23:58:17 cspool450 kernel:  ?<br>\nfilemap_map_pages+0x18f/0x380Apr 16 23:58:17 cspool450<br>\nkernel:  ext4_filemap_fault+0x31/0x44Apr 16 23:58:17 cspool450<br>\nkernel:  __do_fault+0x57/0x117Apr 16 23:58:17 cspool450<br>\nkernel:  __handle_mm_fault+0xda1/0x1230Apr 16 23:58:17 cspool450<br>\nkernel:  handle_mm_fault+0xcb/0x210Apr 16 23:58:17 cspool450<br>\nkernel:  __do_page_fault+0x2a1/0x4d0Apr 16 23:58:17 cspool450<br>\nkernel:  do_page_fault+0x2c/0xe0Apr 16 23:58:17 cspool450<br>\nkernel:  page_fault+0x34/0x40Apr 16 23:58:17 cspool450 kernel: RIP:<br>\n0033:0x5566f01d66a2Apr 16 23:58:17 cspool450 kernel: Code: Bad RIP<br>\nvalue.Apr 16 23:58:17 cspool450 kernel: RSP: 002b:00007fb61fad6790<br>\nEFLAGS: 00010206Apr 16 23:58:17 cspool450 kernel: RAX: 00005566eba0ba18<br>\nRBX: 7fffffffffffffff RCX: 00005566eba0ba18Apr 16 23:58:17 cspool450<br>\nkernel: RDX: 0000132de75dc948 RSI: 000000000001d4ae RDI:<br>\n0000132de75dc8f0Apr 16 23:58:17 cspool450 kernel: RBP: 00007fb61fad67b0<br>\nR08: 0000000000022580 R09: 00007fb61fad6630Apr 16 23:58:17 cspool450<br>\nkernel: R10: 00007fb61fad65f0 R11: 0000000000000206 R12:<br>\n0000132de75dc8f0Apr 16 23:58:17 cspool450 kernel: R13: 00000000000495d4<br>\nR14: 0000132de75dc8f0 R15: 00000020c0b47fedApr 16 23:58:17 cspool450<br>\nkernel: Mem-Info:Apr 16 23:58:17 cspool450 kernel: active_anon:7390182<br>\ninactive_anon:571000<br>\nisolated_anon:0                                   active_file:276<br>\ninactive_file:1<br>\nisolated_file:0                                   unevictable:25173<br>\ndirty:1 writeback:0<br>\nunstable:0                                   slab_reclaimable:25025<br>\nslab_unreclaimable:37942                                   mapped:23813<br>\n7 shmem:516419 pagetables:38113<br>\nbounce:0                                   free:50911 free_pcp:2186<br>\nfree_cma:0Apr 16 23:58:17 cspool450 kernel: Node 0<br>\nactive_anon:29560728kB inactive_anon:2284000kB active_file:1104kB<br>\ninactive_file:4kB unevictable:100692kB isolated(anon):0kB<br>\nisolated(file):0kB mapped:952548kB dirty:4kB writeback:0kB<br>\nshmem:2065676kB shmem_thp: 0kB shmem_pmdmapped: 0kB anon_thp: 0kB<br>\nwriteback_tmp:0kB unstable:0kB all_unreclaimable? noApr 16 23:58:17<br>\ncspool450 kernel: Node 0 DMA free:15900kB min:32kB low:44kB high:56kB<br>\nactive_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB<br>\nunevictable:0kB writepending:0kB present:15992kB managed:15900kB<br>\nmlocked:0kB kernel_stack:0kB pagetables:0kB bounce:0kB free_pcp:0kB<br>\nlocal_pcp:0kB free_cma:0kBApr 16 23:58:17 cspool450 kernel:<br>\nlowmem_reserve[]: 0 1773 31843 31843 31843Apr 16 23:58:17 cspool450<br>\nkernel: Node 0 DMA32 free:124016kB min:3760kB low:5576kB high:7392kB<br>\nactive_anon:1392156kB inactive_anon:355744kB active_file:76kB<br>\ninactive_file:0kB unevictable:3132kB writepending:0kB present:1948512kB<br>\nmanaged:1882692kB mlocked:0kB kernel_stack:0kB pagetables:316kB<br>\nbounce:0kB free_pcp:112kB local_pcp:4kB free_cma:0kBApr 16 23:58:17<br>\ncspool450 kernel: lowmem_reserve[]: 0 0 30069 30069 30069Apr 16<br>\n23:58:17 cspool450 kernel: Node 0 Normal free:63728kB min:63788kB<br>\nlow:94576kB high:125364kB active_anon:28168572kB<br>\ninactive_anon:1928256kB active_file:484kB inactive_file:612kB<br>\nunevictable:97560kB writepending:4kB present:31399936kB<br>\nmanaged:30799340kB mlocked:264kB kernel_stack:24384kB<br>\npagetables:152136kB bounce:0kB free_pcp:8632kB local_pcp:1032kB<br>\nfree_cma:0kBApr 16 23:58:17 cspool450 kernel: lowmem_reserve[]: 0 0 0 0<br>\n0Apr 16 23:58:17 cspool450 kernel: Node 0 DMA: 1<em>4kB (U) 1</em>8kB (U)<br>\n1<em>16kB (U) 2</em>32kB (U) 1<em>64kB (U) 1</em>128kB (U) 1<em>256kB (U) 0</em>512kB<br>\n1<em>1024kB (U) 1</em>2048kB (M) 3*4096kB (M) = 15900kBApr 16 23:58:17<br>\ncspool450 kernel: Node 0 DMA32: 348<em>4kB (UME) 313</em>8kB (UME) 265*16kB<br>\n(UME) 324<em>32kB (UME) 334</em>64kB (UME) 288<em>128kB (UME) 89</em>256kB (UME)<br>\n45<em>512kB (UME) 2</em>1024kB (UM) 0<em>2048kB 0</em>4096kB = 124616kBApr 16<br>\n23:58:17 cspool450 kernel: Node 0 Normal: 5111<em>4kB (UM) 1671</em>8kB (UME)<br>\n490<em>16kB (UME) 298</em>32kB (UME) 219<em>64kB (UME) 0</em>128kB 0<em>256kB 0</em>512kB<br>\n0<em>1024kB 0</em>2048kB 0*4096kB = 65204kBApr 16 23:58:17 cspool450 kernel:<br>\nNode 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0<br>\nhugepages_size=1048576kBApr 16 23:58:17 cspool450 kernel: Node 0<br>\nhugepages_total=0 hugepages_free=0 hugepages_surp=0<br>\nhugepages_size=2048kBApr 16 23:58:17 cspool450 kernel: 672338 total<br>\npagecache pagesApr 16 23:58:17 cspool450 kernel: 155339 pages in swap<br>\ncacheApr 16 23:58:17 cspool450 kernel: Swap cache stats: add 1433360,<br>\ndelete 1278008, find 462505/467558Apr 16 23:58:17 cspool450 kernel:<br>\nFree swap  = 0kBApr 16 23:58:17 cspool450 kernel: Total swap =<br>\n999420kBApr 16 23:58:17 cspool450 kernel: 8341110 pages RAMApr 16<br>\n23:58:17 cspool450 kernel: 0 pages HighMem/MovableOnlyApr 16 23:58:17<br>\ncspool450 kernel: 166627 pages reservedApr 16 23:58:17 cspool450<br>\nkernel: 0 pages cma reservedApr 16 23:58:17 cspool450 kernel: 0 pages<br>\nhwpoisonedApr 16 23:58:17 cspool450 kernel: Tasks state (memory values<br>\nin pages):Apr 16 23:58:17 cspool450 kernel: [  pid  ]   uid  tgid<br>\ntotal_vm      rss pgtables_bytes swapents oom_score_adj nameApr 16<br>\n23:58:17 cspool450 kernel:<br>\n[    564]     0   564    45374      349   380928        0             0<br>\nsystemd-journalApr 16 23:58:17 cspool450 kernel:<br>\n[    574]     0   574    26475       56    94208        0             0<br>\nlvmetadApr 16 23:58:17 cspool450 kernel:<br>\n[    582]     0   582    11752      513   122880        0         -1000 <br>\nsystemd-udevdApr 16 23:58:17 cspool450 kernel:<br>\n[    889]   101   889    17827      318   172032        0             0<br>\nsystemd-resolveApr 16 23:58:17 cspool450 kernel: [    891]<br>\n62583   891    36528      141   192512        0             0 systemd-<br>\ntimesynApr 16 23:58:17 cspool450 kernel:<br>\n[    988]     0   988    17708      232   180224        0             0<br>\nsystemd-logindApr 16 23:58:17 cspool450 kernel:<br>\n[    990]     0   990    74256      194   208896        0             0<br>\nboltdApr 16 23:58:17 cspool450 kernel:<br>\n[    997]     0   997   108599      396   356352        0             0<br>\nModemManagerApr 16 23:58:17 cspool450 kernel:<br>\n[    998]     0   998    44408     1990   241664        0             0<br>\nnetworkd-dispatApr 16 23:58:17 cspool450 kernel:<br>\n[    999]     0   999    27637       95   114688        0             0<br>\nirqbalanceApr 16 23:58:17 cspool450 kernel:<br>\n[   1001]     0  1001    46749      209   196608        0             0<br>\nthermaldApr 16 23:58:17 cspool450 kernel:<br>\n[   1006]     0  1006   125919      656   348160        0             0<br>\nudisksdApr 16 23:58:17 cspool450 kernel:<br>\n[   1017]   116  1017    11815       86   126976        0             0<br>\navahi-daemonApr 16 23:58:17 cspool450 kernel:<br>\n[   1022]   102  1022    65758      325   167936        0             0<br>\nrsyslogdApr 16 23:58:17 cspool450 kernel:<br>\n[   1025]     0  1025     9123      102   114688        0             0<br>\nbluetoothdApr 16 23:58:17 cspool450 kernel:<br>\n[   1031]     0  1031     9606       74   118784        0             0<br>\ncronApr 16 23:58:17 cspool450 kernel:<br>\n[   1035]   103  1035    13205      685   143360        0          -900 <br>\ndbus-daemonApr 16 23:58:17 cspool450 kernel:<br>\n[   1047]   116  1047    11768       86   122880        0             0<br>\navahi-daemonApr 16 23:58:17 cspool450 kernel:<br>\n[   1096]     0  1096    11431      280   126976        0             0<br>\nwpa_supplicantApr 16 23:58:17 cspool450 kernel:<br>\n[   1099]     0  1099    73693      262   208896        0             0<br>\naccounts-daemonApr 16 23:58:17 cspool450 kernel:<br>\n[   1102]     0  1102     1137       16    53248        0             0<br>\nacpidApr 16 23:58:17 cspool450 kernel:<br>\n[   1105]     0  1105   160255     1235   466944        0             0<br>\nNetworkManagerApr 16 23:58:17 cspool450 kernel:<br>\n[   1111]     0  1111   548623     2902   425984        0          -900 <br>\nsnapdApr 16 23:58:17 cspool450 kernel:<br>\n[   1390]     0  1390    75962      891   237568        0             0<br>\npolkitdApr 16 23:58:17 cspool450 kernel:<br>\n[   1643]   117  1643    81510     1538   266240        0             0<br>\ncolordApr 16 23:58:17 cspool450 kernel:<br>\n[   1690]     0  1690    48585     1972   26624<br>\n[message truncated]</p>",
        "id": 294827224,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661245038
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOn 17/04/2020 01:12, Peter Lammich wrote:</p>\n<blockquote>\n<p>I got the error again, and this time I investigated it a bit. Linux invoked<br>\nOOM-killer on the poly-process!<br>\nI attached the log, the last 3 messages show which process was selected for<br>\nkilling.<br>\nThere were two Isabelle's running, and OOM-killer picked one.</p>\n</blockquote>\n<p>I did not know anything about the OOM killer yet, but it seems to cause<br>\nproblems occasionally. E.g. see<br>\n<a href=\"https://serverfault.com/questions/101916/turn-off-the-linux-oom-killer-by-default?rq=1\">https://serverfault.com/questions/101916/turn-off-the-linux-oom-killer-by-default?rq=1</a></p>\n<blockquote>\n<p>Can I somehow tell poly to do more aggressive garbage collection, BEFORE it<br>\ngets OOM killed?</p>\n</blockquote>\n<p>You need to ask that David Matthews.</p>\n<p>Maybe you just need more swap space.</p>\n<p>Or you should limit the poly process explicitly, e.g. in<br>\n$ISABELLE_HOME_USER/etc/settings:</p>\n<p>ML_OPTIONS=\"--minheap 1500 --maxheap 8g\"</p>\n<p>Makarius</p>",
        "id": 294827270,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661245062
    }
]