[
    {
        "content": "<p>From: \"Eugene W. Stark\" &lt;<a href=\"mailto:isabelle-users@starkeffect.com\">isabelle-users@starkeffect.com</a>&gt;<br>\nI have posted (and seen postings) about this general type of thing here before,<br>\nbut I am getting a more refined view of what seems to me to be a problem and want<br>\nto ask again and provide some data points.</p>\n<p>I am running Isabelle 2016 under Ubuntu 14.04 LTS.  I am using the 64-bit<br>\nversion on two similar machines, both of which are Intel(R) Core(TM)<br>\ni5-4670K CPU @ 3.40GHz (4 cores).  One machine has 24GB RAM, and I have<br>\nlimited the ML heap to 16GB to avoid spamming other activities, and the other<br>\nmachine has 16GB RAM, and I have limited the ML heap to 8GB for the same<br>\nreason.  The problem I am describing occurs on both machines, but for definiteness,<br>\nthe screenshots I took today were done on the 16GB machine.</p>\n<p>The problem is that, during sledgehammering using \"try\" it seems that ML will<br>\ngo \"incommunicado\" for minutes at a time, and it is hard to figure out why.<br>\nDuring this period the CPU usage ranges anywhere from 100% to 400%, where 100%<br>\nmeans one core.  Based on previous postings, I would normally assume that it is<br>\ndue to a major GC taking place, but also during these periods, the \"Monitor\"<br>\nwindow in JEdit does not receive any data updates from ML.  Once ML has come<br>\nback online, the Monitor window starts receiving data again, but it interpolates<br>\nthe missing data by a line.  This makes it difficult to understand exactly what<br>\nis going on.  I don't really know whether there are any conditions under which<br>\nit is supposed to be the case that ML will stop sending data points to the<br>\nMonitor window, but it seems odd to me.  Of course, the long pause for an<br>\nunknown period is also frustrating, and in many cases I just terminate and<br>\nrestart Isabelle, substituting the known wait for my theories to be reloaded and<br>\nrechecked for the unknown wait for ML to return.</p>\n<p>For what it's worth, Isabelle memory use is limited, it is honoring the limits,<br>\npaging traffic is low, and CPU utilization is high, so it is not a thrashing<br>\nsituation that is taking place.</p>\n<p>I am attaching some screenshots that I took today when I let the sledghammering<br>\nrun for awhile through multiple cycles of this behavior.<br>\n<a href=\"/user_uploads/14278/aULM1mLi6Bj74HiXoHJeyzMf/Screenshot-from-2016-04-21-102001.png\">Screenshot from 2016-04-21 10:20:01.png</a><br>\n<a href=\"/user_uploads/14278/SwOI0GOeAF3jQSBjLkwf3sqP/Screenshot-from-2016-04-21-102016.png\">Screenshot from 2016-04-21 10:20:16.png</a><br>\n<a href=\"/user_uploads/14278/OTsbBgn40ttwGXKVMyDsl0D1/Screenshot-from-2016-04-21-102027.png\">Screenshot from 2016-04-21 10:20:27.png</a><br>\n<a href=\"/user_uploads/14278/9vvqOuKCRZPStGLYgQ-bekFQ/Screenshot-from-2016-04-21-102037.png\">Screenshot from 2016-04-21 10:20:37.png</a><br>\n<a href=\"/user_uploads/14278/XHtgdsFR0sY9MMsy-w0s5Vwm/Screenshot-from-2016-04-21-102048.png\">Screenshot from 2016-04-21 10:20:48.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/14278/aULM1mLi6Bj74HiXoHJeyzMf/Screenshot-from-2016-04-21-102001.png\" title=\"Screenshot from 2016-04-21 10:20:01.png\"><img src=\"/user_uploads/14278/aULM1mLi6Bj74HiXoHJeyzMf/Screenshot-from-2016-04-21-102001.png\"></a></div><div class=\"message_inline_image\"><a href=\"/user_uploads/14278/SwOI0GOeAF3jQSBjLkwf3sqP/Screenshot-from-2016-04-21-102016.png\" title=\"Screenshot from 2016-04-21 10:20:16.png\"><img src=\"/user_uploads/14278/SwOI0GOeAF3jQSBjLkwf3sqP/Screenshot-from-2016-04-21-102016.png\"></a></div><div class=\"message_inline_image\"><a href=\"/user_uploads/14278/OTsbBgn40ttwGXKVMyDsl0D1/Screenshot-from-2016-04-21-102027.png\" title=\"Screenshot from 2016-04-21 10:20:27.png\"><img src=\"/user_uploads/14278/OTsbBgn40ttwGXKVMyDsl0D1/Screenshot-from-2016-04-21-102027.png\"></a></div><div class=\"message_inline_image\"><a href=\"/user_uploads/14278/9vvqOuKCRZPStGLYgQ-bekFQ/Screenshot-from-2016-04-21-102037.png\" title=\"Screenshot from 2016-04-21 10:20:37.png\"><img src=\"/user_uploads/14278/9vvqOuKCRZPStGLYgQ-bekFQ/Screenshot-from-2016-04-21-102037.png\"></a></div><div class=\"message_inline_image\"><a href=\"/user_uploads/14278/XHtgdsFR0sY9MMsy-w0s5Vwm/Screenshot-from-2016-04-21-102048.png\" title=\"Screenshot from 2016-04-21 10:20:48.png\"><img src=\"/user_uploads/14278/XHtgdsFR0sY9MMsy-w0s5Vwm/Screenshot-from-2016-04-21-102048.png\"></a></div>",
        "id": 294672735,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661173893
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOn Thu, 21 Apr 2016, Eugene W. Stark wrote:</p>\n<blockquote>\n<p>The problem is that, during sledgehammering using \"try\" it seems that ML <br>\nwill go \"incommunicado\" for minutes at a time, and it is hard to figure <br>\nout why.</p>\n</blockquote>\n<blockquote>\n<p>On the snapshot from the \"Heap\" window, you can see spikes in memory <br>\nusage, leading to what appears to be a major GC.  Upon completion of the <br>\nmajor GC, the \"freeze\" sets in, and no data is sent to the monitor for <br>\nseveral minutes. When the data resumes, the heapsize is much lower, but <br>\nquickly spikes up to the max and the cycle repeats.  Because there is no <br>\ndata during the freeze period, it is difficult to tell what is going on, <br>\nbut I have to wonder what it is doing during those 3+ minutes when there <br>\nis only a short spike of computing activity that takes place before it <br>\nstarts again.</p>\n</blockquote>\n<blockquote>\n<p>Could anyone comment on whether this behavior is considered \"normal\"?</p>\n</blockquote>\n<p>It sounds indeed like a normal consequence of a situation where the ML <br>\nheap is under high pressure: tons of data produced and the GC + heap <br>\ncompaction phases trying to reclaim memory.</p>\n<p>When the Poly/ML runtime system performs such GC-related operations, all <br>\nML threads are stopped. The Monitor data is produced by such a regular ML <br>\nthread, so nothing will be reported during that time.</p>\n<blockquote>\n<p>I am suspicious of some kind of thread signalling/synchronization issue, <br>\nbut not knowing anything about the actual implementation, can only <br>\nspeculate.</p>\n</blockquote>\n<blockquote>\n<p>From the description of the problem, it does not really look like that. <br>\nAlthough there is always a possibility for something really strange going <br>\non.</p>\n</blockquote>\n<p>In conclusion, my guess from a distance is that \"sledgehammer\" or \"try\" <br>\nproduce more data than expected, potentially due to unusually big <br>\ntheories. You could also try to reset the machine-learning information in <br>\n$ISABELLE_HOME_USER/mash_state.</p>\n<p>Since you are using 64bit Poly/ML with substantial amounts of heap space, <br>\nthe question is what makes the theories so big that this is required. Even <br>\nthe biggest applications on AFP still fit into the small memory model of <br>\n32bit with 3.5 GB heap. There is a penalty of approx. factor 2 in memory <br>\nrequirements, when switching from 32bit to 64bit.</p>\n<p>Makarius</p>",
        "id": 294672798,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661173917
    },
    {
        "content": "<p>From: Mathias Fleury &lt;<a href=\"mailto:mathias.fleury12@gmail.com\">mathias.fleury12@gmail.com</a>&gt;<br>\nHello all,</p>\n<p>I have seen the described behaviour too. So far, I know that:<br>\nit is linked to MaSh (i.e., MePo does not exhibit this slow-down). You can deactivate the learning in the Sledgehammer options (via the Isabelle options in Isabelle/jEdit or sledgehammer_params).<br>\nremoving the $ISABELLE_HOME_USER/mash_state file helps.</p>\n<p>I have not yet found a deterministic way to reproduce it.</p>\n<p>Mathias</p>",
        "id": 294673036,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661173986
    }
]