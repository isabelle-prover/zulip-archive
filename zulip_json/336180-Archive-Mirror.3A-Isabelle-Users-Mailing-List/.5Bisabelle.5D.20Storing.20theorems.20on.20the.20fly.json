[
    {
        "content": "<p>From: Ognjen Maric &lt;<a href=\"mailto:ognjen.maric@gmail.com\">ognjen.maric@gmail.com</a>&gt;<br>\nHi all,</p>\n<p>I'm writing some Isabelle/ML code which, given a term, generates<br>\ntheorems about it. This is essentially a recursive function which<br>\ngenerates theorems for the subterms and then combines them. A single<br>\nsubterm might appear many times, and I'd like to store the generated<br>\ntheorems to avoid repeating the work. Clearly, they could be manually<br>\nthreaded through the recursive calls, but I'd like to avoid doing the<br>\nplumbing myself, especially since I'd like to be able to retrieve them<br>\nlater (outside of the function) as well.</p>\n<p>So I assume I should be using one of the context.*_Data structures to<br>\nstore the theorems in an Item_Net (indexed by the terms). Questions:</p>\n<p>Questions:</p>\n<ol>\n<li>\n<p>Right now I'm trying to do this from an ML {* *} block within Isar<br>\ntext, using Generic_Data and then storing the generated theorems with<br>\nContext.&gt;&gt;. However, it appears that the theorems only actually get<br>\nstored after the whole ML {* *} block is executed. Is this a feature,<br>\nand if so, how to get around to it?</p>\n</li>\n<li>\n<p>Could this be done within a proof tactic, and how?</p>\n</li>\n</ol>\n<p>Thanks,<br>\nOgnjen</p>",
        "id": 294227018,
        "sender_full_name": "Email Gateway",
        "timestamp": 1660900463
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOn Wed, 28 Nov 2012, Ognjen Maric wrote:</p>\n<blockquote>\n<p>A single subterm might appear many times, and I'd like to store the <br>\ngenerated theorems to avoid repeating the work.</p>\n</blockquote>\n<p>This part in isolation sounds like an application of Thm.cterm_cache or <br>\nThm.thm_cache, i.e. it is semantically plain function application with <br>\noperational tuning for re-use earlier values (in a thread-safe way, with a <br>\nlittle overhead).</p>\n<blockquote>\n<p>Clearly, they could be manually threaded through the recursive calls, <br>\nbut I'd like to avoid doing the plumbing myself, especially since I'd <br>\nlike to be able to retrieve them later (outside of the function) as <br>\nwell.</p>\n<p>So I assume I should be using one of the context.*_Data structures to<br>\nstore the theorems in an Item_Net (indexed by the terms). Questions:</p>\n<p>Questions:<br>\n1. Right now I'm trying to do this from an ML {* *} block within Isar<br>\ntext, using Generic_Data and then storing the generated theorems with<br>\nContext.&gt;&gt;. However, it appears that the theorems only actually get<br>\nstored after the whole ML {* *} block is executed. Is this a feature,<br>\nand if so, how to get around to it?</p>\n</blockquote>\n<p>Sounds all too much like implicit destructive stateful programming to me, <br>\ni.e. not what you normally do in Isabelle/ML.</p>\n<p>What are you trying to achieve concretely?</p>\n<p>The normal way is to \"thread through\" values, usually using the <br>\nIsabelle/ML combinators for that (variations on |&gt; and fold/map/fold_map <br>\nas explained in the implementation manual). This also avoids the seeming <br>\nContext.&gt;&gt; problem above (ML commands have their own linear context that <br>\ncan be updated implicitly -- at compile-time, not run-time).</p>\n<blockquote>\n<ol start=\"2\">\n<li>Could this be done within a proof tactic, and how?</li>\n</ol>\n</blockquote>\n<p>Probably not.  Depends what you actually need.  Tactical proof steps <br>\ncannot change the context.  (I am very glad for that, now that I know how <br>\nmessy this is in other major provers on the market.)</p>\n<p>Makarius</p>",
        "id": 294227022,
        "sender_full_name": "Email Gateway",
        "timestamp": 1660900467
    },
    {
        "content": "<p>From: Ognjen Maric &lt;<a href=\"mailto:ognjen.maric@gmail.com\">ognjen.maric@gmail.com</a>&gt;<br>\nOn 11/28/2012 01:44 PM, Makarius wrote:</p>\n<blockquote>\n<p>This part in isolation sounds like an application of Thm.cterm_cache or<br>\nThm.thm_cache, i.e. it is semantically plain function application with<br>\noperational tuning for re-use earlier values (in a thread-safe way, with<br>\na little overhead).</p>\n</blockquote>\n<p>I'm not immediately certain if they will work in my scenario, but they<br>\nlook very useful, thanks. I didn't know they were there.</p>\n<blockquote>\n<p>Sounds all too much like implicit destructive stateful programming to<br>\nme, i.e. not what you normally do in Isabelle/ML.</p>\n<p>What are you trying to achieve concretely?</p>\n<p>The normal way is to \"thread through\" values, usually using the<br>\nIsabelle/ML combinators for that (variations on |&gt; and fold/map/fold_map<br>\nas explained in the implementation manual). This also avoids the seeming<br>\nContext.&gt;&gt; problem above (ML commands have their own linear context that<br>\ncan be updated implicitly -- at compile-time, not run-time).</p>\n</blockquote>\n<p>They can of course be threaded through, but I haven't been able to come<br>\nup with an elegant way to express it in this case. A stateful solution<br>\nwould be straightforward, though (that is, once you offload all the<br>\nworries about potential concurrency issues and whatnot to somebody else<br>\n:) At any rate, it's just an annoyance, not a showstopper.</p>\n<p>Concretely, I prove refinement between monadic programs (I'm aware of<br>\nPeter's AFP entry). In my particular case, the data refinement typically<br>\naffects a few primitive \"procedures\", for which I prove refinement<br>\n(manually). More complex procedures are then built by combining these<br>\nwith monadic operations. Their refinement proofs are straightforward<br>\n(and tedious). Basically unfold their definitions, build refinement<br>\ntheorems for the components, combine them appropriately and fold the<br>\ndefinitions inside the resulting theorems. It is these theorems I'd like<br>\nto cache (since the \"intermediate\" procedures can appear many times),<br>\nand eventually also export into the context, so that they can be used<br>\nlater on in other \"top-level\" proofs.</p>\n<blockquote>\n<p>Probably not.  Depends what you actually need.  Tactical proof steps<br>\ncannot change the context.  (I am very glad for that, now that I know<br>\nhow messy this is in other major provers on the market.)</p>\n</blockquote>\n<p>Thanks, this isn't such an important point for my application anyway.</p>\n<p>Ognjen</p>",
        "id": 294227034,
        "sender_full_name": "Email Gateway",
        "timestamp": 1660900473
    },
    {
        "content": "<p>From: Makarius &lt;<a href=\"mailto:makarius@sketis.net\">makarius@sketis.net</a>&gt;<br>\nOn Wed, 28 Nov 2012, Ognjen Maric wrote:</p>\n<blockquote>\n<p>They can of course be threaded through, but I haven't been able to come <br>\nup with an elegant way to express it in this case. A stateful solution <br>\nwould be straightforward, though (that is, once you offload all the <br>\nworries about potential concurrency issues and whatnot to somebody else <br>\n:) At any rate, it's just an annoyance, not a showstopper.</p>\n</blockquote>\n<p>I wouldn't use \"stateful\" and \"straightforward\" in the same sentence. <br>\nEven without the omnipresent parallelism in Isabelle, it is better to say <br>\nexplicitly how you fiddle with your values and updated environment.</p>\n<p>See again section 0.3 \"Canonical argument order\" in <br>\n<a href=\"http://isabelle.in.tum.de/dist/Isabelle2012/doc/implementation.pdf\">http://isabelle.in.tum.de/dist/Isabelle2012/doc/implementation.pdf</a> -- it <br>\nis one of the success stories of Isabelle/ML.  We gradually eloborated <br>\nthis purely functional combinator style for aesthetic reasons over the <br>\nyears, and later we found ourselves in the lucky situation to be able to <br>\nget on the multicore train early, with minimal cleanup of the code base.</p>\n<p>Sequentialism is a very strong assumption that is better not imposed on <br>\nany serious program today.</p>\n<blockquote>\n<p>Concretely, I prove refinement between monadic programs (I'm aware of <br>\nPeter's AFP entry). In my particular case, the data refinement typically <br>\naffects a few primitive \"procedures\", for which I prove refinement <br>\n(manually). More complex procedures are then built by combining these <br>\nwith monadic operations. Their refinement proofs are straightforward <br>\n(and tedious). Basically unfold their definitions, build refinement <br>\ntheorems for the components, combine them appropriately and fold the <br>\ndefinitions inside the resulting theorems. It is these theorems I'd like <br>\nto cache (since the \"intermediate\" procedures can appear many times), <br>\nand eventually also export into the context, so that they can be used <br>\nlater on in other \"top-level\" proofs.</p>\n</blockquote>\n<p>I've seen the AFP entry Refine_Monadic occasionally from a distance, but <br>\nam not familiar with it.</p>\n<p>Generally, it depends a lot how you want to organize things, or rather how <br>\nyou need to organize things.</p>\n<p>Within a tactical expression or Isar proof method, you can access the <br>\nProof.context only as a local immutable value (of course it can be updated <br>\nin a functional manner).  The context might contain funny quasi-functional <br>\nelements, such as lazy values or caches of values, but that is <br>\nsemantically still functional.  You cannot have a tactic do some ad-hoc <br>\npoking in a non-monotonic manner.</p>\n<p>Within a proof, you can have commands updating the context according to <br>\nits block structure.  The most basic approach is to make an attribute (see <br>\nthe 'attribute_setup' command in the isar-ref manual) and use that with <br>\n'note' or other fact producing commands in the proof.  Whatever you do, it <br>\nwill be local to the proof.  Proofs are formally irrelevant, i.e. you <br>\ncannot export anything from them.  The main result is specified before, <br>\nand already determined before commencing the proof.</p>\n<p>Within the theory context, you can have commands of the form \"theory -&gt; <br>\ntheory\" or \"local_theory -&gt; local_theory\".  You can store whatever you <br>\nwant in the theory context, as long as it is plain functional data in any <br>\nof the senses above.</p>\n<p>You could try to move results you discover within a proof back to the <br>\ntoplevel context, by exporting it in some logical obvious way.</p>\n<p>In a trivial manner, this happens all the time when you work with <br>\nIsabelle:</p>\n<p>lemma B<br>\n       .<br>\n        .<br>\n         .<br>\n         fix x<br>\n         assume \"A x\"<br>\n         .<br>\n         txt {* Oh, I need \"lemma C x\" *}</p>\n<p>At that point you merely go back in the text and prepend this <em>before</em> it:</p>\n<p>lemma<br>\n     fixes x<br>\n     assumes \"A x\"<br>\n     shows \"C x\"</p>\n<p>with its proof.  So you re-assemble the text in the editor, without funny <br>\ntricks about implicit context extension in the middle of the other proof.</p>\n<p>In more specific, or more complex situations, one could consider making <br>\nthis assembly of properly ordered proof documents supported by some tool. <br>\nIt is up to your imagination.</p>\n<p>Makarius</p>",
        "id": 294227041,
        "sender_full_name": "Email Gateway",
        "timestamp": 1660900480
    },
    {
        "content": "<p>From: Peter Lammich &lt;<a href=\"mailto:lammich@in.tum.de\">lammich@in.tum.de</a>&gt;<br>\nSounds interesting. I have a similar problem, when trying to<br>\nautomatically refine terms to more executable versions (replacing sets<br>\nby red-black-trees, etc.). Once the basic operations are proven,<br>\nrefining more complex terms is straightforward. However, in order to<br>\nrefine a term that contains defined constants (procedures in your<br>\nspeaking), one either has to prove a separate refinement lemma for each<br>\nconstant manually, or unfold all such constants beforehand (one then<br>\ngets one big term, and looses the structure), or do something like you<br>\npropose, i.e., extract those constants from the term first, and<br>\nautomatically generate refined versions.</p>\n<p>We have experimented with the first two options already, and they are<br>\nnot too bad for our use-cases. For the third option, I thought about<br>\nimplementing it as an outer-level command, something like: <br>\n  \"refine constant as name\", that would generate the refinement of<br>\nconstant (and, recursively, of all constants used in the term that need<br>\nto be unfolded), then define a new constant name, and generate a<br>\nrefinement lemma name.refine: \"new-constant refines constant\".</p>\n<p>Note the similarity of this problem to code generation: Also the<br>\ncode-generator collects code-theorems for constants and, recursively,<br>\nfor constants that are used in those code-theorems.</p>",
        "id": 294227133,
        "sender_full_name": "Email Gateway",
        "timestamp": 1660900523
    }
]