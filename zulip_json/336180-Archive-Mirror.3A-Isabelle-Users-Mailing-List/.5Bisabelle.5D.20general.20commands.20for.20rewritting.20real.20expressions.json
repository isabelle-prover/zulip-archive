[
    {
        "content": "<p>From: noam neer &lt;<a href=\"mailto:noamneer@gmail.com\">noamneer@gmail.com</a>&gt;<br>\nHi everybody.</p>\n<p>Whenever I have a rewriting problem (usually involving real expressions) I<br>\ntry first few general commands like<br>\n    by auto<br>\n    by simp<br>\n    by (simp add: field_simps)<br>\n    by sledgehammer</p>\n<p>Sometimes none work, for example for<br>\n    lemma \"(x::real)&gt;0 ==&gt;<br>\n                   ((1+x)<em>(1+x)</em>(1+x) powr (1/3))<br>\n                   =<br>\n                   1+x\"</p>\n<p>In such cases I sometimes decompose the problem to a few simpler ones, and<br>\nsometimes delve into the HOL libraries looking for something useful to add<br>\nto simp, possibly reversed.</p>\n<p>Since these solutions are somewhat tedious, my question is - are there<br>\nother general commands I can try at this point? Or maybe something like<br>\n'field_simps' that is more appropriate for real expressions? Any advice<br>\nwould be appreciated.</p>\n<p>Thanx.</p>",
        "id": 294734131,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661189181
    },
    {
        "content": "<p>From: Lawrence Paulson &lt;<a href=\"mailto:lp15@cam.ac.uk\">lp15@cam.ac.uk</a>&gt;<br>\nI'm afraid there is no magic bullet for this sort of problem. I often find divide_simps to be more effective than field_simps, but not here. It is obvious how to make that particular example easier to prove. In general though we are all struggling to prove obvious things.</p>\n<p>Larry Paulson</p>",
        "id": 294734501,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661189309
    },
    {
        "content": "<p>From: Lukas Bulwahn &lt;<a href=\"mailto:lukas.bulwahn@gmail.com\">lukas.bulwahn@gmail.com</a>&gt;<br>\nHi Noam,</p>\n<p>In addition to Larry's answer:</p>\n<p>I believe you should be made aware of try, try0 and sledgehammer,<br>\nthese commands invoke generally powerful methods; they are not silver<br>\nbullets but are as automatic as it gets in Isabelle and in interactive<br>\ntheorem provers in general.</p>\n<p>Other than that, you have the right strategy of decomposing the goal,<br>\nlooking for useful theorems and trying to identify useful lemma sets,<br>\nsuch as field_simps, divide_simps etc.</p>\n<p>Lukas</p>",
        "id": 294734553,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661189328
    },
    {
        "content": "<p>From: \"Eugene W. Stark\" &lt;<a href=\"mailto:isabelle-users@starkeffect.com\">isabelle-users@starkeffect.com</a>&gt;<br>\nThere are many others on this list with much more experience than I have,<br>\nbut FWIW here are some techniques that I have found useful:</p>\n<p>* \"try\" is quite useful, not only for finding out about relevant facts,<br>\n     but also for finding out what proof methods are most likely to work<br>\n     in various situations.  Surprisingly often, \"try\" will come up with<br>\n     a proof if you let it run for a little while.  A disadvantage is that<br>\n     sometimes it seems to explode and cause the system to embark on very<br>\n     long garbage collections that put you out of business for many minutes<br>\n     (this is a major peeve I have, which I wish could be improved).<br>\n     Other times it will claim to produce a proof, but you will find that<br>\n     it doesn't actually work.  However, by looking at the list of facts<br>\n     that it thinks are relevant, you can select the ones you agree with,<br>\n     add them to your \"using\" list, and re-run \"try\".  Sometimes you can<br>\n     converge on a proof in this way.  At least you are getting an idea<br>\n     of what facts the system has that might be relevant.  \"Try\" will<br>\n     quite often find startling proofs by \"smt\" that are mostly<br>\n     incomprehensible by just looking at the list of facts used.<br>\n     In working out first versions of a development, I will use these<br>\n     as a quick demonstration that what I am trying to prove is actually<br>\n     true, but later I will go back and expand into something understandable.<br>\n     Infrequently, you can replace \"smt\" by \"metis\" or \"metis (no_types, lifting)\"<br>\n     to avoid the use of \"smt\".  Sometimes \"try\" will claim to produce proofs<br>\n     by \"smt\" that mention \"everything but the kitchen sink\" and will say<br>\n     that \"smt\" timed out.  I throw these away and try something else.</p>\n<p>* \"try0\" is useful if you have a pretty good idea of a superset of the<br>\n     facts needed to prove something, but you don't know which proof method<br>\n     is going to work.  The \"try0\" command has a time limit, which avoids<br>\n     spamming the system like \"try\" can do, so it is not a bad idea to try<br>\n     it first, \"just in case\".  A disadvantage of \"try0\" is that it will miss<br>\n     long-running proofs by \"force\" and \"fastforce\" because they will time out.<br>\n     That is, you might have a sufficient list of facts, but a proof by<br>\n     \"force\" or \"fastforce\" will take longer than \"try0\" allows.  This can<br>\n     be frustrating.</p>\n<p>* As a first strategy when automatic attempts to find a proof fail,<br>\n     look at the output from using \"apply simp\" on your goal.<br>\n     Try to get an idea why the simplifier only gets as far as it does.<br>\n     This can help to identify missing hypotheses or to try to get an<br>\n     idea of what kind of additional facts should be supplied.<br>\n     If \"apply simp\" makes substantial progress, but fails to complete<br>\n     the proof, take the result it produces and work on it as a subgoal.<br>\n     If you succeed in proving this subgoal, then you can complete the<br>\n     original proof by adding one additional \"by simp\" step.</p>\n<p>* If a goal refuses to succumb to your first attempts, your main recourse is<br>\n     to try to break it into simpler subgoals.  The output of \"apply auto\"<br>\n     is one way of doing this, but the goals might not be very natural.<br>\n     In other cases, you have to apply your understanding of how the proof should<br>\n     go to figure out how to reduce the original goal to subgoals that are likely<br>\n     to be easier to solve.  Sometimes if you look around you can find introduction<br>\n     or elimination rules that were not applied automatically, and you can explicitly<br>\n     invoke them in the first step of the proof.</p>\n<p>* Another strategy that can be helpful when you think you have a set of facts that<br>\n     ought to prove the result but you can't see why the system will not find a proof,<br>\n     is to start specifying the required instances of the facts using the [of ...]<br>\n     construct.  While doing this, keep an eye on the output window to make sure that<br>\n     you are in fact specifying the correct instances (this can be done by comparing<br>\n     terms in the instances of the facts being used with terms in the goal).<br>\n     If you get the instances wrong, then you will waste a lot of time on a proof that<br>\n     can't work, so it is best to be careful here.<br>\n     In some cases, specifying partial or full instantiations will provide the boost<br>\n     needed for one of the proof methods to succeed.  In general, \"force\" and \"fastforce\"<br>\n     often require that instances be specified, whereas \"auto\" and \"simp\" seem to<br>\n     be able to find the required instances automatically more often.<br>\n     If I find a proof in this way, I then generally attempt to reduce the instance<br>\n     information to the bare minimum needed for the proof to succeed.  The reason for<br>\n     this is, even though the instance information is useful documentation for a<br>\n     human reader, if a change is made later to the facts that have been instantiated,<br>\n     the order in which the terms have to be specified will often change, which<br>\n     will cause the proof to break.  So a proof with as few instances specified as<br>\n     possible will generally be more robust to later changes in the statements of the<br>\n     facts it uses.</p>\n<p>* One of the most difficult things for me in trying to learn how to<br>\n     produce proofs in the system was to come to understand the \"personalities\"<br>\n     of the various proof methods.  Probably knowing how each of them is<br>\n     implemented would be very useful, but I don't have such knowledge,<br>\n     and probably most normal users wouldn't, either.  From trying to do proofs<br>\n     lots of different ways and using \"try\" and observing the results,<br>\n     the following seem to me to be true:</p>\n<p>* \"simp\" is perhaps the main workhorse.  It takes facts, treats<br>\n      them as rewrite rules, applies the rules to the goal, accumulates<br>\n      consequences of the rules, and ultimately attempts to rewrite<br>\n      the goal to \"True\".  When equations are used as rules, the<br>\n      orientation is important: if the rules do not produce something<br>\n      \"simpler\" (it can be hard to figure out what this ought to mean<br>\n      in a given situation) then \"simp\" will not work well.<br>\n      \"Simp\" can loop when presented with incompatibly oriented rules;<br>\n      the symptom of this is \"by simp\" remaining a purple color, rather<br>\n      than turning pink (indicating failure) after some time.<br>\n          One can sometimes glean some information about why the looping<br>\n          has occurred by adding [[simp_trace]] and perhaps also<br>\n      [[simp_trace_depth_limit=5]] (or whatever depth is desired)<br>\n          to the \"using\" list and then hovering the mouse over the \"by simp\"<br>\n      or \"apply simp\" to see the simplifier trace.  The output can be<br>\n      voluminous and is not that easy to understand.</p>\n<p>Another important thing to understand about \"simp\" is how it<br>\n      uses implications P1 ==&gt; P2 ==&gt; ... ==&gt; Q as rewrite rules.<br>\n          It uses such an implication by unifying Q with the goal, then<br>\n      recursively attempting to simplify the obtained instances of<br>\n      P1, P2, ..., to \"True\".  If a subgoal fails to simplify, then<br>\n      backtracking occurs, and the simplifier gives up on that rule<br>\n      and tries another.  As far as I can see, the simplifier considers<br>\n      \"success\" to have occurred whenever <em>some</em> simplification is made.<br>\n          Once this occurs, backtracking is apparently suppressed for that<br>\n          subgoal.  What this means is that sometimes having \"too many\"<br>\n      simplification rules can cause the simplifier to miss things<br>\n      that it actually has rules for.  This can seem mysterious unless<br>\n          you are able to examine the simplifier trace, and even then it can<br>\n      take time to figure out.  So it is important to be able to control<br>\n      the facts the simplifier has and when failure occurs to try to<br>\n      understand the reason for it.</p>\n<p>* \"auto\" extends \"simp\" by doing things like splitting cases and canonicalizing<br>\n      the goal in various ways.  Whereas \"simp\" will not increase the number<br>\n      of subgoals, \"auto\" will often do so.  As far as I know, if something can be<br>\n      proved by \"simp\", it can also be proved by \"auto\", though the proof might<br>\n      take slightly longer.  Just as looking at the output of \"apply simp\"<br>\n      can be helpful to understanding, looking at the output of \"apply auto\"<br>\n      can, as well, though because of the case splitting and other<br>\n      transformations that \"auto\" makes to the goal it can be more difficult<br>\n      to understand the result.</p>\n<p>The output of \"apply simp\" or \"apply auto\" will show a failed subgoal<br>\n      of the form P1 ==&gt; P2 ==&gt; ... ==&gt; Q.  I typically focus first on Q,<br>\n      as that is generally where the essence of what is going on is located.<br>\n      Then work back, looking at the Pi to see among them are rules that<br>\n      would unify with Q and try to see whether there is sufficient information<br>\n      to verify their hypotheses.  In some cases, Q will be obviously untrue,<br>\n      in which case \"False\" needs to be derivable from the Pi.</p>\n<p>* \"force\" and \"fastforce\" will use simplification, but they seem to<br>\n      restrict the simplifications that are used in some sense that I<br>\n      don't really understand well.  They will also generally use introduction<br>\n          and elimination rules in more general contexts than what \"auto\"<br>\n      will do.  For example, \"auto\" seems unwilling to consider the use of<br>\n      an elimination rule if the term to which the rule applies happens to<br>\n      be conjoined with other terms, but it will use the elimination rule<br>\n      if the term stands alone.  On the other hand \"force\" and \"fastforce\"<br>\n      seem to be able to apply the elimination rule regardless.<br>\n      Guessing from the names, \"force\" and \"fastforce\" probably also do a<br>\n      certain amount of brute-force search.  This seems to make them useful<br>\n      in situations where there are permutative facts that need to be applied<br>\n      that have no suitable orientations as rewrite rules.  On the other hand,<br>\n      this means that these methods may succumb to combinatorial explosion in<br>\n      cert<br>\n[message truncated]</p>",
        "id": 294734570,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661189335
    },
    {
        "content": "<p>From: \"Nagashima, Yutaka\" &lt;<a href=\"mailto:Yutaka.Nagashima@uibk.ac.at\">Yutaka.Nagashima@uibk.ac.at</a>&gt;<br>\nHi Noam,</p>\n<p>If neither \"try\" or \"try0\" tries hard enough, why don't you try \"try_hard\" from PSL [1].<br>\nAlternatively, you can write your own proof strategy:</p>\n<p>strategy ForNoam =<br>\n  Thens<br>\n    [Ors <br>\n      [Auto,<br>\n       Simp,<br>\n       User &lt;simp add: field_simps&gt;,<br>\n       User &lt;simp add: devide_simps&gt;,<br>\n       User &lt;simp add: arith&gt;,<br>\n       User &lt;simp add: algebra&gt;,<br>\n       User &lt;simp add: ac_simps&gt;,<br>\n       User &lt;simp add: algebra_simps&gt;,<br>\n       Hammer<br>\n       ],<br>\n     IsSolved<br>\n     ]</p>\n<p>and apply this strategy as follows:</p>\n<p>lemma \"True (<em>just an example</em>)\"<br>\n  find_proof ForNoam<br>\n  oops</p>\n<p>Essentially, this strategy keeps applying </p>\n<ul>\n<li>auto,</li>\n<li>simp,</li>\n<li>simp add: field_simps,</li>\n<li>simp add: devide_simps,</li>\n<li>simp add: arith,</li>\n<li>simp add: algebra,</li>\n<li>simp add: algebra_simps,</li>\n<li>sledgehammer<br>\nuntil one of them discharges your proof obligation.</li>\n</ul>\n<p>If one of them discharges your proof obligation, <br>\nPSL shows that proof method with the corresponding arguments<br>\nin the output window.</p>\n<p>To use PSL, you have to download its source code form GitHub [2] and <br>\nimport \"PSL.thy\" using the \"import\" command as usual with the right<br>\npath to \"PSL.thy\".</p>\n<p>It is just a theory file. So, installation should be easy.</p>\n<p>The drawback of this approach is that it can be pretty slow at finding a proof script.<br>\nBut if it finds a proof script, the proof script shown in the output window tends to be efficient.</p>\n<p>I attached a small example file (Scratch.thy).</p>\n<p>[1] documentation (Springer or arXiv): <br>\n<a href=\"https://doi.org/10.1007/978-3-319-63046-5_32\">https://doi.org/10.1007/978-3-319-63046-5_32</a><br>\n<a href=\"https://arxiv.org/pdf/1606.02941.pdf\">https://arxiv.org/pdf/1606.02941.pdf</a> </p>\n<p>[2] source code: <a href=\"https://github.com/data61/PSL/releases/tag/v0.1.1\">https://github.com/data61/PSL/releases/tag/v0.1.1</a></p>\n<p>Regards,<br>\nYutaka<br>\n<a href=\"/user_uploads/14278/PqfzWga9zVCx4deD5PEwj8J2/Scratch.thy\">Scratch.thy</a></p>",
        "id": 294734686,
        "sender_full_name": "Email Gateway",
        "timestamp": 1661189378
    }
]